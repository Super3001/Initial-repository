{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023.6.26 调整此模型结构，以达到在dataset.go_emotions数据集上的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比试验：\n",
    "\n",
    "tokenize:  word | AutoTokenizer\n",
    "\n",
    "embedding:  pretrain | embedding layer\n",
    "\n",
    "model:      lstm | blstm | transformer\n",
    "\n",
    "stop_words: use | not use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import keyedvectors\n",
    "gz = r\"C:\\Users\\songy\\gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\"\n",
    "vector = r\"C:\\Users\\songy\\Documents\\word2vec.vector\"\n",
    "wv = keyedvectors.load_word2vec_format(gz,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.KeyedVectors"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.save_word2vec_format('word2vec.vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wv['hello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['hello'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 5.75k/5.75k [00:00<?, ?B/s]\n",
      "Downloading metadata: 100%|██████████| 7.03k/7.03k [00:00<?, ?B/s]\n",
      "Downloading readme: 100%|██████████| 9.11k/9.11k [00:00<00:00, 9.76MB/s]\n",
      "No config specified, defaulting to: go_emotions/simplified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset go_emotions/simplified to C:/Users/songy/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 3.52MB [00:01, 2.73MB/s]                            \n",
      "Downloading data: 439kB [00:00, 699kB/s]                             \n",
      "Downloading data: 437kB [00:00, 966kB/s]                            \n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset go_emotions downloaded and prepared to C:/Users/songy/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 99.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# 使用 go_emotions(simplified) 数据集\n",
    "import datasets\n",
    "data = datasets.load_dataset('go_emotions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: go_emotions/simplified\n",
      "Found cached dataset go_emotions (C:/Users/songy/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d)\n",
      "100%|██████████| 3/3 [00:00<00:00, 93.66it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "data = datasets.load_dataset(path=r'C:\\Users\\songy\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\go_emotions\\2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d\\go_emotions.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data['train']\n",
    "data_val = data['validation']\n",
    "data_test = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43410 5426 5427\n"
     ]
    }
   ],
   "source": [
    "print(len(data_train),len(data_val),len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional',\n",
       " '_multi_tensor',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(torch.optim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer使用Adam或者ASGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AdaptiveLogSoftmaxWithLoss',\n",
       " 'BCELoss',\n",
       " 'BCEWithLogitsLoss',\n",
       " 'CTCLoss',\n",
       " 'CosineEmbeddingLoss',\n",
       " 'CrossEntropyLoss',\n",
       " 'GaussianNLLLoss',\n",
       " 'HingeEmbeddingLoss',\n",
       " 'HuberLoss',\n",
       " 'KLDivLoss',\n",
       " 'L1Loss',\n",
       " 'MSELoss',\n",
       " 'MarginRankingLoss',\n",
       " 'MultiLabelMarginLoss',\n",
       " 'MultiLabelSoftMarginLoss',\n",
       " 'MultiMarginLoss',\n",
       " 'NLLLoss',\n",
       " 'NLLLoss2d',\n",
       " 'PoissonNLLLoss',\n",
       " 'SmoothL1Loss',\n",
       " 'SoftMarginLoss',\n",
       " 'TripletMarginLoss',\n",
       " 'TripletMarginWithDistanceLoss']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dir(nn) if 'Loss' in x]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类任务使用CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChainedScheduler',\n",
       " 'ConstantLR',\n",
       " 'CosineAnnealingLR',\n",
       " 'CosineAnnealingWarmRestarts',\n",
       " 'Counter',\n",
       " 'CyclicLR',\n",
       " 'EPOCH_DEPRECATION_WARNING',\n",
       " 'ExponentialLR',\n",
       " 'LRScheduler',\n",
       " 'LambdaLR',\n",
       " 'LinearLR',\n",
       " 'MultiStepLR',\n",
       " 'MultiplicativeLR',\n",
       " 'OneCycleLR',\n",
       " 'Optimizer',\n",
       " 'PolynomialLR',\n",
       " 'ReduceLROnPlateau',\n",
       " 'SequentialLR',\n",
       " 'StepLR',\n",
       " '_LRScheduler',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_enable_get_lr_call',\n",
       " 'bisect_right',\n",
       " 'inf',\n",
       " 'math',\n",
       " 'types',\n",
       " 'warnings',\n",
       " 'weakref',\n",
       " 'wraps']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(torch.optim.lr_scheduler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scheduler使用LinearLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial_1: 搭建单个LSTM模型，使用预训练词库进行embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2030 3024\n",
      "class_label:\n",
      "        names:\n",
      "          '0': admiration\n",
      "          '1': amusement\n",
      "          '2': anger\n",
      "          '3': annoyance\n",
      "          '4': approval\n",
      "          '5': caring\n",
      "          '6': confusion\n",
      "          '7': curiosity\n",
      "          '8': desire\n",
      "          '9': disappointment\n",
      "          '10': disapproval\n",
      "          '11': disgust\n",
      "          '12': embarrassment\n",
      "          '13': excitement\n",
      "          '14': fear\n",
      "          '15': gratitude\n",
      "          '16': grief\n",
      "          '17': joy\n",
      "          '18': love\n",
      "          '19': nervousness\n",
      "          '20': optimism\n",
      "          '21': pride\n",
      "          '22': realization\n",
      "          '23': relief\n",
      "          '24': remorse\n",
      "          '25': sadness\n",
      "          '26': surprise\n",
      "          '27': neutral\n",
      "  - name: id\n",
      "    dtype: string\n",
      "  splits:\n",
      "  - name: train\n",
      "    num_bytes: 4224198\n",
      "    num_examples: 43410\n",
      "  - name: validation\n",
      "    num_bytes: 527131\n",
      "    num_examples: 5426\n",
      "  - name: test\n",
      "    num_bytes: 524455\n",
      "    num_examples: 5427\n",
      "  download_size: 4394818\n",
      "  dataset_size: 5275784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看模型信息\n",
    "md = {}\n",
    "with open(r'C:\\Users\\songy\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\go_emotions\\2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d\\README.md') as md_file:\n",
    "    md['text'] = md_file.read()\n",
    "pos1 = md['text'].find('class_label:')\n",
    "pos2 = md['text'][pos1:].find('---')\n",
    "md['config'] = md['text'][pos1:pos1+pos2]\n",
    "print(md['config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "classes = re.findall(r\"'\\d+': \\w+\",md['config'])\n",
    "class_list = []\n",
    "for each in classes:\n",
    "    class_list.append(each[each.find(': ')+2:])\n",
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [\n",
    "    'admiration',\n",
    "    'amusement',\n",
    "    'anger',\n",
    "    'annoyance',\n",
    "    'approval',\n",
    "    'caring',\n",
    "    'confusion',\n",
    "    'curiosity',\n",
    "    'desire',\n",
    "    'disappointment',\n",
    "    'disapproval',\n",
    "    'disgust',\n",
    "    'embarrassment',\n",
    "    'excitement',\n",
    "    'fear',\n",
    "    'gratitude',\n",
    "    'grief',\n",
    "    'joy',\n",
    "    'love',\n",
    "    'nervousness',\n",
    "    'optimism',\n",
    "    'pride',\n",
    "    'realization',\n",
    "    'relief',\n",
    "    'remorse',\n",
    "    'sadness',\n",
    "    'surprise',\n",
    "    'neutral'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"My favourite food is anything I didn't have to cook myself.\",\n",
       " 'labels': [27],\n",
       " 'id': 'eebbqej'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in wv.index_to_key if '<' in x and 'http' not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>',\n",
       " 'in',\n",
       " 'for',\n",
       " 'that',\n",
       " 'is',\n",
       " 'on',\n",
       " '##',\n",
       " 'The',\n",
       " 'with',\n",
       " 'said',\n",
       " 'was',\n",
       " 'the',\n",
       " 'at',\n",
       " 'not',\n",
       " 'as',\n",
       " 'it',\n",
       " 'be',\n",
       " 'from',\n",
       " 'by',\n",
       " 'are',\n",
       " 'I',\n",
       " 'have',\n",
       " 'he',\n",
       " 'will',\n",
       " 'has',\n",
       " '####',\n",
       " 'his',\n",
       " 'an',\n",
       " 'this',\n",
       " 'or',\n",
       " 'their',\n",
       " 'who',\n",
       " 'they',\n",
       " 'but',\n",
       " '$',\n",
       " 'had',\n",
       " 'year',\n",
       " 'were',\n",
       " 'we',\n",
       " 'more',\n",
       " '###',\n",
       " 'up',\n",
       " 'been',\n",
       " 'you',\n",
       " 'its',\n",
       " 'one',\n",
       " 'about',\n",
       " 'would',\n",
       " 'which',\n",
       " 'out',\n",
       " 'can',\n",
       " 'It',\n",
       " 'all',\n",
       " 'also',\n",
       " 'two',\n",
       " 'after',\n",
       " 'first',\n",
       " 'He',\n",
       " 'do',\n",
       " 'time',\n",
       " 'than',\n",
       " 'when',\n",
       " 'We',\n",
       " 'over',\n",
       " 'last',\n",
       " 'new',\n",
       " 'other',\n",
       " 'her',\n",
       " 'people',\n",
       " 'into',\n",
       " 'In',\n",
       " 'our',\n",
       " 'there',\n",
       " 'A',\n",
       " 'she',\n",
       " 'could',\n",
       " 'just',\n",
       " 'years',\n",
       " 'some',\n",
       " 'U.S.',\n",
       " 'three',\n",
       " 'million',\n",
       " 'them',\n",
       " 'what',\n",
       " 'But',\n",
       " 'so',\n",
       " 'no',\n",
       " 'like',\n",
       " 'if',\n",
       " 'only',\n",
       " 'percent',\n",
       " 'get',\n",
       " 'did',\n",
       " 'him',\n",
       " 'game',\n",
       " 'back',\n",
       " 'because',\n",
       " 'now',\n",
       " '#.#',\n",
       " 'before',\n",
       " 'company',\n",
       " 'any',\n",
       " 'team',\n",
       " 'against',\n",
       " 'off',\n",
       " 'This',\n",
       " 'most',\n",
       " 'made',\n",
       " 'through',\n",
       " 'make',\n",
       " 'second',\n",
       " 'state',\n",
       " 'well',\n",
       " 'day',\n",
       " 'season',\n",
       " 'says',\n",
       " 'week',\n",
       " 'where',\n",
       " 'while',\n",
       " 'down',\n",
       " 'being',\n",
       " 'government',\n",
       " 'your',\n",
       " '#-#',\n",
       " 'home',\n",
       " 'going',\n",
       " 'my',\n",
       " 'good',\n",
       " 'They',\n",
       " \"'re\",\n",
       " 'should',\n",
       " 'many',\n",
       " 'way',\n",
       " 'those',\n",
       " 'four',\n",
       " 'during',\n",
       " 'such',\n",
       " 'may',\n",
       " 'very',\n",
       " 'how',\n",
       " 'since',\n",
       " 'work',\n",
       " 'take',\n",
       " 'including',\n",
       " 'high',\n",
       " 'then',\n",
       " '%',\n",
       " 'next',\n",
       " '#,###',\n",
       " 'By',\n",
       " 'much',\n",
       " 'still',\n",
       " 'go',\n",
       " 'think',\n",
       " 'old',\n",
       " 'even',\n",
       " '#.##',\n",
       " 'world',\n",
       " 'see',\n",
       " 'say',\n",
       " 'business',\n",
       " 'five',\n",
       " 'told',\n",
       " 'under',\n",
       " 'us',\n",
       " '1',\n",
       " 'these',\n",
       " 'If',\n",
       " 'right',\n",
       " 'And',\n",
       " 'me',\n",
       " 'between',\n",
       " 'play',\n",
       " 'help',\n",
       " '##,###',\n",
       " 'market',\n",
       " 'That',\n",
       " 'know',\n",
       " 'end',\n",
       " 'AP',\n",
       " 'long',\n",
       " 'information',\n",
       " 'points',\n",
       " 'does',\n",
       " 'both',\n",
       " 'There',\n",
       " 'part',\n",
       " 'around',\n",
       " 'police',\n",
       " 'want',\n",
       " \"'ve\",\n",
       " 'based',\n",
       " 'For',\n",
       " 'got',\n",
       " 'third',\n",
       " 'school',\n",
       " 'left',\n",
       " 'another',\n",
       " 'country',\n",
       " 'need',\n",
       " '2',\n",
       " 'best',\n",
       " 'win',\n",
       " 'quarter',\n",
       " 'use',\n",
       " 'today',\n",
       " '##.#',\n",
       " 'same',\n",
       " 'public',\n",
       " 'run',\n",
       " 'Friday',\n",
       " 'set',\n",
       " 'month',\n",
       " 'top',\n",
       " 'billion',\n",
       " 'Tuesday',\n",
       " 'come',\n",
       " 'Monday',\n",
       " 'She',\n",
       " 'city',\n",
       " 'place',\n",
       " 'night',\n",
       " 'six',\n",
       " 'each',\n",
       " 'Thursday',\n",
       " '###,###',\n",
       " 'Wednesday',\n",
       " 'here',\n",
       " 'You',\n",
       " 'group',\n",
       " 'really',\n",
       " 'found',\n",
       " 'As',\n",
       " 'used',\n",
       " '3',\n",
       " 'lot',\n",
       " \"'m\",\n",
       " 'money',\n",
       " 'put',\n",
       " 'games',\n",
       " 'support',\n",
       " 'program',\n",
       " 'half',\n",
       " 'report',\n",
       " 'family',\n",
       " 'months',\n",
       " 'number',\n",
       " 'officials',\n",
       " 'am',\n",
       " 'former',\n",
       " 'own',\n",
       " 'man',\n",
       " 'Saturday',\n",
       " 'too',\n",
       " 'better',\n",
       " 'days',\n",
       " 'came',\n",
       " 'lead',\n",
       " 'life',\n",
       " 'American',\n",
       " '##-##',\n",
       " 'show',\n",
       " 'past',\n",
       " 'took',\n",
       " 'added',\n",
       " 'expected',\n",
       " 'called',\n",
       " 'great',\n",
       " 'State',\n",
       " 'services',\n",
       " 'children',\n",
       " 'hit',\n",
       " 'area',\n",
       " 'system',\n",
       " 'every',\n",
       " 'pm',\n",
       " 'big',\n",
       " 'service',\n",
       " 'few',\n",
       " 'per',\n",
       " 'members',\n",
       " 'Sunday',\n",
       " 'early',\n",
       " 'point',\n",
       " 'start',\n",
       " 'companies',\n",
       " 'little',\n",
       " '&',\n",
       " 'case',\n",
       " 'ago',\n",
       " 'local',\n",
       " 'according',\n",
       " 'never',\n",
       " '5',\n",
       " 'without',\n",
       " 'sales',\n",
       " 'until',\n",
       " 'went',\n",
       " 'players',\n",
       " '##th',\n",
       " 'New_York',\n",
       " 'won',\n",
       " 'financial',\n",
       " 'news',\n",
       " '4',\n",
       " 'When',\n",
       " 'share',\n",
       " 'several',\n",
       " 'free',\n",
       " 'away',\n",
       " '##.##',\n",
       " 'already',\n",
       " 'On',\n",
       " 'industry',\n",
       " \"'ll\",\n",
       " 'call',\n",
       " 'With',\n",
       " 'students',\n",
       " 'line',\n",
       " 'available',\n",
       " 'County',\n",
       " 'making',\n",
       " 'held',\n",
       " 'final',\n",
       " '#:##',\n",
       " 'power',\n",
       " 'plan',\n",
       " 'might',\n",
       " 'least',\n",
       " 'look',\n",
       " 'forward',\n",
       " 'give',\n",
       " 'At',\n",
       " 'again',\n",
       " 'later',\n",
       " 'full',\n",
       " 'must',\n",
       " 'things',\n",
       " 'major',\n",
       " 'community',\n",
       " 'announced',\n",
       " 'open',\n",
       " 'record',\n",
       " 'reported',\n",
       " 'court',\n",
       " 'working',\n",
       " 'able',\n",
       " 'something',\n",
       " 'president',\n",
       " 'meeting',\n",
       " 'keep',\n",
       " 'March',\n",
       " 'future',\n",
       " 'far',\n",
       " 'deal',\n",
       " 'City',\n",
       " 'May',\n",
       " 'development',\n",
       " 'University',\n",
       " 'find',\n",
       " 'times',\n",
       " 'After',\n",
       " 'office',\n",
       " 'led',\n",
       " 'among',\n",
       " 'June',\n",
       " 'increase',\n",
       " 'China',\n",
       " 'John',\n",
       " 'whether',\n",
       " 'cost',\n",
       " 'security',\n",
       " 'job',\n",
       " 'less',\n",
       " 'head',\n",
       " 'seven',\n",
       " 'growth',\n",
       " 'lost',\n",
       " 'pay',\n",
       " 'looking',\n",
       " 'provide',\n",
       " '6',\n",
       " 'To',\n",
       " 'plans',\n",
       " 'products',\n",
       " 'car',\n",
       " 'recent',\n",
       " 'hard',\n",
       " 'always',\n",
       " 'include',\n",
       " 'women',\n",
       " 'across',\n",
       " 'tax',\n",
       " 'water',\n",
       " 'April',\n",
       " 'continue',\n",
       " 'important',\n",
       " 'different',\n",
       " 'close',\n",
       " '7',\n",
       " 'One',\n",
       " 'late',\n",
       " 'decision',\n",
       " 'current',\n",
       " 'law',\n",
       " 'within',\n",
       " 'along',\n",
       " 'played',\n",
       " 'move',\n",
       " 'United_States',\n",
       " 'enough',\n",
       " 'become',\n",
       " 'side',\n",
       " 'national',\n",
       " 'Inc.',\n",
       " 'results',\n",
       " 'level',\n",
       " 'loss',\n",
       " 'economic',\n",
       " 'coach',\n",
       " 'near',\n",
       " 'getting',\n",
       " 'price',\n",
       " 'Department',\n",
       " 'event',\n",
       " 'fourth',\n",
       " 'change',\n",
       " 'All',\n",
       " 'small',\n",
       " 'board',\n",
       " 'National',\n",
       " 'So',\n",
       " 'goal',\n",
       " 'taken',\n",
       " 'field',\n",
       " 'prices',\n",
       " 'weeks',\n",
       " 'men',\n",
       " 'asked',\n",
       " 'eight',\n",
       " 'data',\n",
       " 'shot',\n",
       " 'New',\n",
       " 'started',\n",
       " 'July',\n",
       " 'director',\n",
       " 'President',\n",
       " 'party',\n",
       " 'federal',\n",
       " 'done',\n",
       " 'political',\n",
       " 'minutes',\n",
       " 'taking',\n",
       " 'Company',\n",
       " 'technology',\n",
       " 'project',\n",
       " 'center',\n",
       " 'leading',\n",
       " 'issue',\n",
       " 'though',\n",
       " 'having',\n",
       " 'period',\n",
       " 'likely',\n",
       " 'scored',\n",
       " '8',\n",
       " 'strong',\n",
       " 'series',\n",
       " 'military',\n",
       " 'seen',\n",
       " 'trying',\n",
       " 'What',\n",
       " 'coming',\n",
       " 'process',\n",
       " 'building',\n",
       " 'behind',\n",
       " 'performance',\n",
       " 'management',\n",
       " 'Iraq',\n",
       " 'saying',\n",
       " 'earlier',\n",
       " 'believe',\n",
       " 'oil',\n",
       " 'given',\n",
       " 'Police',\n",
       " 'customers',\n",
       " 'due',\n",
       " 'following',\n",
       " 'term',\n",
       " 'others',\n",
       " 'statement',\n",
       " 'international',\n",
       " 'economy',\n",
       " 'health',\n",
       " 'thing',\n",
       " 'Obama',\n",
       " 'return',\n",
       " 'killed',\n",
       " 'Washington',\n",
       " 'further',\n",
       " 'However',\n",
       " 'doing',\n",
       " 'face',\n",
       " 'low',\n",
       " 'higher',\n",
       " 'site',\n",
       " 'once',\n",
       " 'yet',\n",
       " 'hours',\n",
       " 'America',\n",
       " 'control',\n",
       " 'received',\n",
       " 'rate',\n",
       " 'career',\n",
       " 'Bush',\n",
       " 'teams',\n",
       " 'known',\n",
       " 'offer',\n",
       " 'race',\n",
       " 'ever',\n",
       " 'experience',\n",
       " 'playing',\n",
       " 'name',\n",
       " 'possible',\n",
       " 'countries',\n",
       " 'Mr.',\n",
       " 'average',\n",
       " 'together',\n",
       " 'using',\n",
       " '9',\n",
       " 'cut',\n",
       " 'While',\n",
       " 'total',\n",
       " 'round',\n",
       " 'young',\n",
       " 'nearly',\n",
       " 'shares',\n",
       " 'member',\n",
       " 'campaign',\n",
       " 'media',\n",
       " 'needs',\n",
       " 'why',\n",
       " 'house',\n",
       " 'issues',\n",
       " 'costs',\n",
       " 'fire',\n",
       " '##-#',\n",
       " 'victory',\n",
       " 'player',\n",
       " 'began',\n",
       " 'sure',\n",
       " 'story',\n",
       " 'per_cent',\n",
       " 'North',\n",
       " 'His',\n",
       " 'staff',\n",
       " 'order',\n",
       " 'war',\n",
       " 'large',\n",
       " 'interest',\n",
       " 'stock',\n",
       " 'food',\n",
       " 'research',\n",
       " 'key',\n",
       " 'India',\n",
       " 'South',\n",
       " 'morning',\n",
       " 'conference',\n",
       " 'senior',\n",
       " 'global',\n",
       " 'Center',\n",
       " 'death',\n",
       " 'person',\n",
       " 'thought',\n",
       " 'gave',\n",
       " 'feel',\n",
       " 'energy',\n",
       " 'history',\n",
       " 'recently',\n",
       " 'largest',\n",
       " 'No.',\n",
       " 'general',\n",
       " 'official',\n",
       " 'released',\n",
       " 'wanted',\n",
       " 'meet',\n",
       " 'short',\n",
       " 'outside',\n",
       " 'running',\n",
       " 'live',\n",
       " 'ball',\n",
       " 'online',\n",
       " 'real',\n",
       " 'position',\n",
       " 'fact',\n",
       " 'fell',\n",
       " 'nine',\n",
       " 'December',\n",
       " 'front',\n",
       " 'action',\n",
       " 'defense',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'Mr',\n",
       " 'nation',\n",
       " 'needed',\n",
       " 'special',\n",
       " 'January',\n",
       " 'almost',\n",
       " 'chance',\n",
       " \"'d\",\n",
       " 'result',\n",
       " 'West',\n",
       " 'September',\n",
       " 'reports',\n",
       " 'leader',\n",
       " 'investment',\n",
       " 'yesterday',\n",
       " 'Some',\n",
       " 'leaders',\n",
       " 'ahead',\n",
       " 'production',\n",
       " 'comes',\n",
       " 'No',\n",
       " 'runs',\n",
       " 'match',\n",
       " 'role',\n",
       " 'kind',\n",
       " 'try',\n",
       " 'ended',\n",
       " 'risk',\n",
       " 'areas',\n",
       " 'election',\n",
       " 'workers',\n",
       " 'visit',\n",
       " 'bring',\n",
       " 'road',\n",
       " 'music',\n",
       " 'study',\n",
       " 'makes',\n",
       " 'often',\n",
       " 'release',\n",
       " 'woman',\n",
       " 'vote',\n",
       " 'care',\n",
       " 'town',\n",
       " 'clear',\n",
       " 'comment',\n",
       " 'budget',\n",
       " 'potential',\n",
       " 'single',\n",
       " 'markets',\n",
       " 'policy',\n",
       " 'capital',\n",
       " 'saw',\n",
       " 'access',\n",
       " 'weekend',\n",
       " 'operations',\n",
       " 'whose',\n",
       " 'net',\n",
       " 'House',\n",
       " 'hand',\n",
       " 'increased',\n",
       " 'charges',\n",
       " 'winning',\n",
       " 'trade',\n",
       " 'These',\n",
       " 'income',\n",
       " 'value',\n",
       " 'involved',\n",
       " 'Bank',\n",
       " 'November',\n",
       " 'bill',\n",
       " 'compared',\n",
       " 'anything',\n",
       " 'manager',\n",
       " 'Texas',\n",
       " 'property',\n",
       " 'stop',\n",
       " 'annual',\n",
       " 'private',\n",
       " 'contract',\n",
       " 'died',\n",
       " 'Now',\n",
       " 'hope',\n",
       " 'product',\n",
       " 'fans',\n",
       " 'lower',\n",
       " 'demand',\n",
       " 'News',\n",
       " 'David',\n",
       " 'club',\n",
       " 'comments',\n",
       " 'film',\n",
       " 'yards',\n",
       " 'quality',\n",
       " 'currently',\n",
       " 'events',\n",
       " 'addition',\n",
       " 'couple',\n",
       " 'schools',\n",
       " 'attack',\n",
       " 'region',\n",
       " 'latest',\n",
       " 'opportunity',\n",
       " 'worked',\n",
       " 'course',\n",
       " 'bad',\n",
       " 'fall',\n",
       " 'Group',\n",
       " 'October',\n",
       " 'jobs',\n",
       " 'list',\n",
       " 'let',\n",
       " 'however',\n",
       " 'chief',\n",
       " 'summer',\n",
       " 'programs',\n",
       " 'According',\n",
       " 'revenue',\n",
       " 'Our',\n",
       " 'rose',\n",
       " 'previous',\n",
       " 'TV',\n",
       " 'football',\n",
       " 'biggest',\n",
       " 'employees',\n",
       " 'changes',\n",
       " 'residents',\n",
       " 'means',\n",
       " 'agreement',\n",
       " 'includes',\n",
       " 'post',\n",
       " 'Canada',\n",
       " 'probably',\n",
       " 'related',\n",
       " 'training',\n",
       " 'allowed',\n",
       " 'class',\n",
       " 'bit',\n",
       " 'video',\n",
       " 'Michael',\n",
       " 'An',\n",
       " 'sent',\n",
       " 'education',\n",
       " 'states',\n",
       " 'straight',\n",
       " 'love',\n",
       " 'beat',\n",
       " 'hold',\n",
       " 'turn',\n",
       " 'finished',\n",
       " 'network',\n",
       " 'Smith',\n",
       " 'buy',\n",
       " 'foreign',\n",
       " 'especially',\n",
       " 'groups',\n",
       " 'wants',\n",
       " 'title',\n",
       " 'included',\n",
       " 'turned',\n",
       " 'bank',\n",
       " 'Florida',\n",
       " 'efforts',\n",
       " 'personal',\n",
       " 'businesses',\n",
       " 'August',\n",
       " 'California',\n",
       " 'situation',\n",
       " 'district',\n",
       " 'allow',\n",
       " 'helped',\n",
       " 'body',\n",
       " 'nothing',\n",
       " 'soon',\n",
       " 'safety',\n",
       " 'officer',\n",
       " 'cents',\n",
       " 'Europe',\n",
       " 'St.',\n",
       " 'additional',\n",
       " 'spokesman',\n",
       " 'February',\n",
       " 'wife',\n",
       " 'showed',\n",
       " 'leave',\n",
       " 'investors',\n",
       " 'parents',\n",
       " 'medical',\n",
       " 'spending',\n",
       " 'non',\n",
       " 'London',\n",
       " 'Council',\n",
       " 'matter',\n",
       " 'spent',\n",
       " 'child',\n",
       " 'World',\n",
       " 'effort',\n",
       " 'opening',\n",
       " 'either',\n",
       " 'range',\n",
       " 'question',\n",
       " 'European',\n",
       " 'goals',\n",
       " 'administration',\n",
       " 'friends',\n",
       " 'himself',\n",
       " 'shows',\n",
       " 'difficult',\n",
       " 'kids',\n",
       " 'paid',\n",
       " 'create',\n",
       " 'cash',\n",
       " 'age',\n",
       " 'league',\n",
       " 'form',\n",
       " 'impact',\n",
       " 'drive',\n",
       " 'someone',\n",
       " 'became',\n",
       " 'stay',\n",
       " 'fight',\n",
       " 'significant',\n",
       " 'firm',\n",
       " 'Senate',\n",
       " 'hospital',\n",
       " 'charged',\n",
       " 'operating',\n",
       " 'main',\n",
       " 'book',\n",
       " 'success',\n",
       " 'son',\n",
       " 'trading',\n",
       " '###-####',\n",
       " 'focus',\n",
       " 'room',\n",
       " 'continued',\n",
       " 'Congress',\n",
       " 'everything',\n",
       " 'Park',\n",
       " 'agency',\n",
       " 'brought',\n",
       " 'talk',\n",
       " 'break',\n",
       " 'air',\n",
       " 'software',\n",
       " 'decided',\n",
       " 'Do',\n",
       " 'ready',\n",
       " 'arrested',\n",
       " 'track',\n",
       " 'provides',\n",
       " 'mother',\n",
       " 'base',\n",
       " 'trial',\n",
       " 'phone',\n",
       " 'My',\n",
       " 'build',\n",
       " 'conditions',\n",
       " 'rest',\n",
       " 'Johnson',\n",
       " 'terms',\n",
       " 'expect',\n",
       " 'England',\n",
       " 'Israel',\n",
       " 'despite',\n",
       " 'closed',\n",
       " 'starting',\n",
       " 'provided',\n",
       " 'pressure',\n",
       " 'lives',\n",
       " 'step',\n",
       " 'remain',\n",
       " 'similar',\n",
       " 'charge',\n",
       " 'date',\n",
       " 'whole',\n",
       " 'land',\n",
       " 'growing',\n",
       " 'James',\n",
       " 'Internet',\n",
       " 'projects',\n",
       " 'British',\n",
       " 'cases',\n",
       " 'ground',\n",
       " 'legal',\n",
       " 'International',\n",
       " 'agreed',\n",
       " 'tell',\n",
       " 'test',\n",
       " 'everyone',\n",
       " 'pretty',\n",
       " 'authorities',\n",
       " 'Two',\n",
       " 'above',\n",
       " 'moved',\n",
       " 'profit',\n",
       " 'throughout',\n",
       " 'inside',\n",
       " 'ability',\n",
       " 'overall',\n",
       " 'pass',\n",
       " 'officers',\n",
       " 'rather',\n",
       " 'Australia',\n",
       " 'actually',\n",
       " 'county',\n",
       " 'amount',\n",
       " 'scheduled',\n",
       " 'themselves',\n",
       " 'organization',\n",
       " 'giving',\n",
       " 'credit',\n",
       " 'father',\n",
       " 'drug',\n",
       " 'investigation',\n",
       " 'families',\n",
       " 'Republican',\n",
       " 'funds',\n",
       " 'patients',\n",
       " 'takes',\n",
       " 'systems',\n",
       " 'Japan',\n",
       " 'complete',\n",
       " 'sold',\n",
       " 'practice',\n",
       " 'calls',\n",
       " '•',\n",
       " 'UK',\n",
       " 'force',\n",
       " 'student',\n",
       " 'idea',\n",
       " 'reached',\n",
       " 'reason',\n",
       " 'levels',\n",
       " 'space',\n",
       " 'competition',\n",
       " 'forces',\n",
       " 'sector',\n",
       " 'Last',\n",
       " 'tried',\n",
       " 'common',\n",
       " 'homes',\n",
       " 'stage',\n",
       " 'department',\n",
       " 'named',\n",
       " 'earnings',\n",
       " 'offers',\n",
       " 'star',\n",
       " 'certain',\n",
       " 'double',\n",
       " 'longer',\n",
       " 'followed',\n",
       " 'cause',\n",
       " 'Association',\n",
       " 'signed',\n",
       " 'committee',\n",
       " 'hour',\n",
       " 'college',\n",
       " 'Pakistan',\n",
       " 'users',\n",
       " 'Iran',\n",
       " 'sign',\n",
       " 'living',\n",
       " 'failed',\n",
       " 'reach',\n",
       " 'quickly',\n",
       " 'receive',\n",
       " 'debt',\n",
       " 'sale',\n",
       " 'Board',\n",
       " 'Americans',\n",
       " 'Road',\n",
       " 'Brown',\n",
       " 'insurance',\n",
       " '##:##',\n",
       " 'anyone',\n",
       " 'tournament',\n",
       " 'More',\n",
       " 'gas',\n",
       " 'talks',\n",
       " 'serious',\n",
       " 'required',\n",
       " 'sell',\n",
       " 'construction',\n",
       " 'evidence',\n",
       " 'remains',\n",
       " 'black',\n",
       " 'below',\n",
       " 'improve',\n",
       " 'crisis',\n",
       " 'address',\n",
       " 'questions',\n",
       " 'easy',\n",
       " 'begin',\n",
       " 'view',\n",
       " 'School',\n",
       " 'heard',\n",
       " 'executive',\n",
       " 'raised',\n",
       " ...]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.1291504e-03, -8.9645386e-04,  3.1852722e-04,  1.5335083e-03,\n",
       "        1.1062622e-03, -1.4038086e-03, -3.0517578e-05, -4.1961670e-04,\n",
       "       -5.7601929e-04,  1.0757446e-03, -1.0223389e-03, -6.1798096e-04,\n",
       "       -7.5531006e-04,  1.4038086e-03, -1.6403198e-03, -6.3323975e-04,\n",
       "        1.6326904e-03, -1.0070801e-03, -1.2664795e-03,  6.5231323e-04,\n",
       "       -4.1580200e-04, -1.0757446e-03,  1.5258789e-03, -2.7465820e-04,\n",
       "        1.4019012e-04,  1.5716553e-03,  1.3580322e-03, -8.3160400e-04,\n",
       "       -1.4038086e-03,  1.5792847e-03,  2.5367737e-04, -7.3242188e-04,\n",
       "       -1.0538101e-04, -1.1672974e-03,  1.5792847e-03,  6.5612793e-04,\n",
       "       -6.5994263e-04,  2.9206276e-06,  1.1291504e-03,  4.2724609e-04,\n",
       "       -3.7002563e-04, -1.1520386e-03,  1.2664795e-03, -3.5166740e-06,\n",
       "        2.6512146e-04, -4.0245056e-04,  1.4114380e-04, -3.3617020e-05,\n",
       "        7.5912476e-04, -5.1879883e-04, -7.1048737e-05,  6.0272217e-04,\n",
       "       -5.0735474e-04, -1.6250610e-03, -4.3678284e-04, -9.9182129e-04,\n",
       "       -1.2207031e-03, -3.2234192e-04,  6.8664551e-05, -1.1672974e-03,\n",
       "       -5.1116943e-04,  1.4114380e-03,  3.3569336e-04, -4.7492981e-04,\n",
       "       -1.3732910e-03,  3.6621094e-04, -1.4419556e-03, -6.0653687e-04,\n",
       "        8.0108643e-04,  1.1291504e-03, -8.3541870e-04, -1.1596680e-03,\n",
       "        9.1552734e-04,  5.2261353e-04, -3.2806396e-04,  1.5945435e-03,\n",
       "       -1.5792847e-03, -3.5667419e-04,  4.9591064e-04,  1.0147095e-03,\n",
       "       -1.0986328e-03, -1.6593933e-04, -1.4209747e-04, -2.6130676e-04,\n",
       "        1.2588501e-03,  3.8623810e-05,  1.6880035e-04, -1.0299683e-03,\n",
       "        1.6098022e-03,  6.2942505e-04,  4.1770935e-04, -1.3504028e-03,\n",
       "        3.4904480e-04,  1.1444092e-03, -1.2054443e-03, -1.1825562e-03,\n",
       "        9.4985962e-04,  6.0558319e-05,  1.0728836e-05, -6.6757202e-04,\n",
       "        1.2435913e-03,  6.9046021e-04,  5.5551529e-05, -8.6212158e-04,\n",
       "       -1.1672974e-03,  1.2130737e-03, -8.0490112e-04, -8.7738037e-04,\n",
       "        2.2792816e-04, -3.9672852e-04, -8.5830688e-04,  2.8800964e-04,\n",
       "       -1.5869141e-03,  4.8446655e-04, -1.1215210e-03,  1.9669533e-06,\n",
       "       -3.7956238e-04,  7.0571899e-04, -1.5869141e-03,  1.6250610e-03,\n",
       "        1.5563965e-03, -4.3106079e-04,  9.8419189e-04,  9.0408325e-04,\n",
       "       -1.3961792e-03,  1.2054443e-03, -7.0190430e-04,  2.7084351e-04,\n",
       "       -1.2359619e-03,  6.9046021e-04, -8.4304810e-04,  1.3427734e-03,\n",
       "       -1.4343262e-03, -6.7138672e-04,  1.5487671e-03, -1.0986328e-03,\n",
       "        1.1901855e-03, -1.4266968e-03, -6.8283081e-04, -7.8582764e-04,\n",
       "        4.8065186e-04,  4.0817261e-04, -6.3705444e-04,  1.4495850e-04,\n",
       "       -9.7656250e-04,  1.4495850e-03,  8.4564090e-07, -1.6632080e-03,\n",
       "       -3.2806396e-04,  6.2942505e-04, -1.4343262e-03, -3.4141541e-04,\n",
       "        1.1520386e-03, -5.3024292e-04, -4.7111511e-04, -8.5067749e-04,\n",
       "       -1.3809204e-03, -1.2435913e-03, -1.3275146e-03,  1.0757446e-03,\n",
       "        1.3198853e-03, -3.0326843e-04, -3.7431717e-05,  1.1825562e-03,\n",
       "       -1.3580322e-03, -1.0452271e-03,  5.6743622e-05, -1.0147095e-03,\n",
       "        4.3296814e-04, -1.5716553e-03, -9.1075897e-05,  1.0604858e-03,\n",
       "       -6.0272217e-04, -1.5335083e-03, -1.5335083e-03,  5.4168701e-04,\n",
       "        1.3351440e-03,  4.1198730e-04, -3.1089783e-04,  1.7642975e-04,\n",
       "       -1.3732910e-04, -6.9808960e-04, -8.6212158e-04, -1.0833740e-03,\n",
       "       -2.9802322e-05,  8.0108643e-04,  6.7901611e-04,  3.3569336e-04,\n",
       "       -1.3885498e-03,  1.3504028e-03,  2.3460388e-04, -1.3351440e-03,\n",
       "       -8.7356567e-04, -7.4386597e-04,  1.0833740e-03,  6.0558319e-05,\n",
       "       -1.2664795e-03,  1.1901855e-03, -6.2179565e-04,  1.3597310e-07,\n",
       "        1.2741089e-03, -9.8419189e-04, -1.5487671e-03,  1.5563965e-03,\n",
       "       -1.3122559e-03, -7.9345703e-04,  1.5335083e-03,  1.2969971e-03,\n",
       "       -1.8024445e-04,  9.1934204e-04,  1.2054443e-03,  7.7056885e-04,\n",
       "       -1.6555786e-03,  7.7056885e-04,  1.4495850e-03, -1.3046265e-03,\n",
       "        6.1035156e-04,  6.5994263e-04,  1.2588501e-03,  1.4190674e-03,\n",
       "       -1.2207031e-03, -1.5106201e-03,  1.1291504e-03,  1.3427734e-03,\n",
       "        1.6632080e-03, -5.7220459e-04, -5.5694580e-04,  3.9863586e-04,\n",
       "       -2.7084351e-04,  4.9591064e-04,  1.6098022e-03, -7.0571899e-04,\n",
       "        6.2561035e-04, -9.7656250e-04, -1.8978119e-04,  9.5367432e-05,\n",
       "       -5.1879883e-04, -2.0408630e-04, -8.2778931e-04, -1.2302399e-04,\n",
       "        7.6293945e-04,  3.2234192e-04, -1.2435913e-03,  9.9182129e-04,\n",
       "        1.0604858e-03, -1.4114380e-03,  9.6797943e-05, -1.5563965e-03,\n",
       "        2.1934509e-04, -5.5313110e-05, -9.1171265e-04, -1.4877319e-03,\n",
       "        1.3656616e-03, -8.4304810e-04, -4.1961670e-04,  3.2424927e-04,\n",
       "       -1.0070801e-03,  1.2588501e-04, -4.5585632e-04,  1.9264221e-04,\n",
       "       -2.6893616e-04,  1.4953613e-03, -1.5869141e-03,  5.9127808e-04,\n",
       "       -1.4648438e-03,  9.6511841e-04, -1.2817383e-03,  1.6021729e-03,\n",
       "        1.0910034e-03, -1.3122559e-03,  1.0910034e-03, -5.1116943e-04,\n",
       "        3.4523010e-04,  1.0452271e-03, -2.0694733e-04,  9.0408325e-04,\n",
       "        6.6757202e-04,  1.1062622e-03, -8.7356567e-04, -3.7574768e-04,\n",
       "       -2.5749207e-04, -9.1552734e-05,  1.4343262e-03, -1.1825562e-03,\n",
       "       -8.7261200e-05,  1.3275146e-03, -1.5830994e-04,  1.2893677e-03,\n",
       "       -9.8419189e-04, -5.4931641e-04, -1.5487671e-03,  1.3732910e-03,\n",
       "       -6.0796738e-05, -8.2397461e-04,  1.3275146e-03,  1.1596680e-03,\n",
       "        5.6838989e-04, -1.5640259e-03, -1.2302399e-04, -8.6307526e-05],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.1-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\anaconda3\\lib\\site-packages (from gensim) (1.20.3)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: scipy>=1.7.0 in d:\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.1 smart-open-6.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No config specified, defaulting to: go_emotions/simplified\n",
      "Found cached dataset go_emotions (C:/Users/songy/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d)\n",
      "100%|██████████| 3/3 [00:00<00:00, 78.93it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"init()\"\"\"\n",
    "from gensim.models import keyedvectors\n",
    "gz = r\"C:\\Users\\songy\\gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\"\n",
    "vector = r\"C:\\Users\\songy\\Documents\\word2vec.vector\"\n",
    "wv = keyedvectors.load_word2vec_format(gz,binary=True)\n",
    "\n",
    "# 使用 go_emotions(simplified) 数据集\n",
    "import datasets\n",
    "data = datasets.load_dataset(path=r'C:\\Users\\songy\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\go_emotions\\2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d\\go_emotions.py')\n",
    "data_train = data['train']\n",
    "data_val = data['validation']\n",
    "data_test = data['test']\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 查看模型信息\n",
    "# with open('go_emotions_labels.txt', 'r') as f:\n",
    "#     classes = f.readlines()\n",
    "from go_emotions_classes import classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favourite food is anything I didn't have to cook myself.\n",
      "favourite not in wv\n",
      "to not in wv\n",
      "myself. not in wv\n",
      "[array([-0.0534668 ,  0.13085938,  0.265625  ,  0.20898438, -0.234375  ,\n",
      "        0.14355469,  0.10302734, -0.13964844, -0.01025391,  0.13378906,\n",
      "        0.05688477,  0.140625  ,  0.03320312,  0.06030273, -0.35351562,\n",
      "        0.04003906,  0.32617188, -0.11621094, -0.00460815, -0.21191406,\n",
      "        0.03564453,  0.19921875,  0.39648438,  0.00372314, -0.08056641,\n",
      "       -0.05273438, -0.2421875 , -0.11523438,  0.2109375 , -0.15722656,\n",
      "        0.18457031, -0.08154297, -0.02416992,  0.15039062,  0.03613281,\n",
      "       -0.0390625 , -0.06176758,  0.27734375, -0.00671387, -0.10400391,\n",
      "        0.0201416 ,  0.06396484,  0.10546875,  0.03063965,  0.15625   ,\n",
      "        0.06298828, -0.11914062, -0.06787109,  0.29882812,  0.11767578,\n",
      "       -0.07373047,  0.17578125,  0.07373047, -0.08544922,  0.0480957 ,\n",
      "        0.28710938, -0.22558594, -0.00244141, -0.11376953,  0.01501465,\n",
      "       -0.12695312, -0.01904297,  0.15039062,  0.06933594, -0.15527344,\n",
      "        0.06445312,  0.0402832 , -0.10205078, -0.05419922,  0.05249023,\n",
      "        0.13183594, -0.14550781,  0.09375   ,  0.26367188, -0.24707031,\n",
      "       -0.06689453,  0.15039062, -0.08007812,  0.32617188,  0.22363281,\n",
      "        0.03833008, -0.35351562,  0.25585938,  0.14257812, -0.23242188,\n",
      "       -0.00741577, -0.20605469,  0.22167969, -0.07958984,  0.03198242,\n",
      "       -0.09033203,  0.07958984, -0.0035553 ,  0.08496094, -0.22363281,\n",
      "        0.26757812,  0.25195312,  0.13867188, -0.01416016,  0.13671875,\n",
      "       -0.1953125 , -0.25390625, -0.08740234, -0.3671875 ,  0.00549316,\n",
      "        0.0859375 , -0.15039062,  0.203125  , -0.1328125 , -0.17382812,\n",
      "       -0.36328125, -0.12695312,  0.0255127 ,  0.26757812,  0.33398438,\n",
      "        0.12988281, -0.15332031,  0.02954102, -0.00897217, -0.11328125,\n",
      "       -0.25585938, -0.13867188, -0.06933594,  0.16796875, -0.02819824,\n",
      "       -0.06103516, -0.26953125, -0.18457031, -0.05712891, -0.05981445,\n",
      "       -0.37304688, -0.28125   , -0.00482178,  0.1328125 ,  0.12060547,\n",
      "       -0.29296875, -0.28710938,  0.11083984,  0.07226562,  0.14648438,\n",
      "        0.00540161,  0.20703125, -0.19433594, -0.2265625 , -0.05639648,\n",
      "       -0.203125  , -0.16601562, -0.21875   , -0.00830078, -0.02966309,\n",
      "        0.06298828,  0.00343323,  0.16210938,  0.125     ,  0.00601196,\n",
      "        0.01263428, -0.08300781, -0.14160156, -0.03662109, -0.06689453,\n",
      "        0.02807617, -0.27148438,  0.12402344,  0.40039062,  0.07177734,\n",
      "       -0.22265625, -0.02307129, -0.02429199,  0.0324707 , -0.09716797,\n",
      "       -0.39453125,  0.17089844,  0.04833984, -0.32617188, -0.30664062,\n",
      "        0.359375  ,  0.10058594, -0.1875    , -0.06396484,  0.0071106 ,\n",
      "       -0.08740234, -0.07080078, -0.04345703,  0.24316406,  0.00946045,\n",
      "        0.234375  , -0.33789062,  0.01074219,  0.07763672, -0.24121094,\n",
      "       -0.20898438,  0.05932617,  0.10986328,  0.12109375, -0.04492188,\n",
      "       -0.09960938, -0.18652344,  0.02868652, -0.05664062, -0.3359375 ,\n",
      "       -0.18457031, -0.20898438, -0.01385498,  0.21875   , -0.04101562,\n",
      "        0.02294922, -0.26757812,  0.328125  ,  0.08300781,  0.10595703,\n",
      "       -0.07177734,  0.26171875, -0.07373047,  0.03344727, -0.08740234,\n",
      "       -0.05688477,  0.13867188,  0.07763672, -0.2421875 ,  0.09960938,\n",
      "       -0.04248047,  0.07568359,  0.10107422, -0.01287842,  0.19921875,\n",
      "       -0.390625  , -0.07177734,  0.06787109, -0.16015625, -0.07080078,\n",
      "        0.12695312, -0.34570312,  0.41601562,  0.06054688,  0.0546875 ,\n",
      "        0.12011719, -0.05200195, -0.04785156, -0.39257812,  0.03564453,\n",
      "       -0.03271484, -0.17285156, -0.02722168, -0.04101562, -0.01275635,\n",
      "       -0.05541992, -0.24316406,  0.27148438, -0.08935547,  0.08642578,\n",
      "        0.07763672, -0.09326172,  0.06445312,  0.296875  ,  0.07470703,\n",
      "       -0.07421875,  0.01745605, -0.359375  , -0.12353516, -0.11572266,\n",
      "       -0.09423828,  0.05053711,  0.01989746, -0.265625  ,  0.01818848,\n",
      "        0.27929688, -0.24023438, -0.01226807, -0.07763672,  0.11035156,\n",
      "       -0.40039062,  0.25      ,  0.02404785,  0.0324707 , -0.01696777,\n",
      "       -0.16210938,  0.17089844, -0.08837891,  0.01409912, -0.171875  ,\n",
      "        0.22167969, -0.12988281,  0.14355469,  0.36132812,  0.1953125 ,\n",
      "        0.20800781, -0.0456543 , -0.02722168, -0.26367188,  0.15429688,\n",
      "       -0.17871094,  0.20898438, -0.28125   , -0.23730469, -0.12207031,\n",
      "       -0.22460938, -0.11279297, -0.00415039,  0.21972656, -0.17773438],\n",
      "      dtype=float32), 0, array([-0.18164062,  0.16503906, -0.16601562,  0.35742188, -0.09228516,\n",
      "        0.20117188, -0.0546875 , -0.26171875, -0.17285156, -0.08056641,\n",
      "        0.14648438, -0.24609375,  0.18652344,  0.10253906, -0.3203125 ,\n",
      "        0.16699219, -0.0032196 , -0.06640625,  0.06591797, -0.109375  ,\n",
      "        0.13964844, -0.05029297,  0.25390625,  0.0859375 ,  0.02026367,\n",
      "        0.05517578, -0.08447266,  0.07324219,  0.15429688, -0.13867188,\n",
      "       -0.25195312, -0.15136719,  0.07958984,  0.00848389, -0.24902344,\n",
      "        0.05224609,  0.04394531, -0.19726562, -0.2109375 ,  0.01477051,\n",
      "       -0.23632812, -0.14355469,  0.17773438,  0.26757812, -0.08789062,\n",
      "       -0.07910156, -0.16113281,  0.23632812, -0.07177734,  0.08837891,\n",
      "        0.07177734, -0.11962891, -0.09228516, -0.12060547, -0.00448608,\n",
      "       -0.21875   , -0.05712891, -0.04418945,  0.07226562, -0.05883789,\n",
      "       -0.12597656,  0.03125   , -0.24609375,  0.19140625,  0.14941406,\n",
      "       -0.19335938, -0.1875    , -0.05126953,  0.03369141, -0.21679688,\n",
      "        0.05273438, -0.13183594,  0.04101562, -0.00500488, -0.21484375,\n",
      "       -0.18066406, -0.36914062,  0.05957031,  0.02075195, -0.18457031,\n",
      "        0.12011719, -0.18359375,  0.23144531,  0.05810547, -0.23730469,\n",
      "       -0.15039062, -0.20410156,  0.07617188, -0.0234375 , -0.09521484,\n",
      "        0.06787109, -0.05566406, -0.07324219, -0.36328125, -0.00579834,\n",
      "       -0.10742188,  0.00964355, -0.00224304,  0.00427246, -0.00512695,\n",
      "       -0.06640625,  0.17285156,  0.23730469,  0.19433594, -0.00976562,\n",
      "       -0.06542969, -0.02868652, -0.23242188, -0.18554688, -0.03662109,\n",
      "       -0.20898438, -0.30078125, -0.10839844, -0.03540039, -0.18066406,\n",
      "        0.10693359,  0.13671875, -0.01599121, -0.09814453,  0.0055542 ,\n",
      "       -0.3203125 ,  0.09033203, -0.25976562, -0.15039062, -0.11230469,\n",
      "        0.08886719,  0.01080322, -0.04418945, -0.04418945, -0.10253906,\n",
      "       -0.17382812,  0.04541016, -0.20507812,  0.05932617, -0.22753906,\n",
      "        0.34179688,  0.28515625, -0.12255859,  0.14453125, -0.01843262,\n",
      "        0.140625  , -0.10986328, -0.18457031, -0.1171875 , -0.08251953,\n",
      "       -0.07080078,  0.14941406,  0.19628906, -0.36328125, -0.02526855,\n",
      "        0.05102539,  0.25976562, -0.203125  , -0.04394531,  0.04125977,\n",
      "       -0.10009766, -0.06835938,  0.05322266, -0.375     , -0.25195312,\n",
      "       -0.16796875, -0.04931641, -0.05761719,  0.23535156, -0.06591797,\n",
      "        0.05297852, -0.14550781, -0.00145721,  0.12109375, -0.21582031,\n",
      "       -0.41210938,  0.03088379,  0.14257812, -0.09130859, -0.12109375,\n",
      "       -0.07080078,  0.07763672,  0.0291748 , -0.04589844, -0.02355957,\n",
      "       -0.29296875, -0.19238281,  0.10302734, -0.1171875 ,  0.08935547,\n",
      "       -0.07373047, -0.11914062,  0.04272461, -0.05712891,  0.11328125,\n",
      "        0.04150391,  0.11621094,  0.125     , -0.0390625 , -0.08496094,\n",
      "        0.24121094,  0.17675781, -0.25585938, -0.19433594, -0.12011719,\n",
      "       -0.0559082 , -0.03417969, -0.0255127 , -0.22949219, -0.24609375,\n",
      "        0.00946045,  0.18066406, -0.05493164,  0.06176758,  0.03027344,\n",
      "       -0.28125   , -0.05004883,  0.15527344,  0.10888672,  0.19140625,\n",
      "        0.0088501 ,  0.11865234,  0.09228516, -0.05175781,  0.05200195,\n",
      "        0.20019531, -0.08691406,  0.16210938,  0.03125   ,  0.20800781,\n",
      "        0.16894531, -0.03198242,  0.01177979,  0.09033203,  0.07617188,\n",
      "       -0.05810547,  0.19726562, -0.10986328,  0.11425781, -0.00257874,\n",
      "       -0.18066406,  0.3046875 ,  0.02893066, -0.01525879, -0.06542969,\n",
      "        0.18847656,  0.17285156,  0.13867188,  0.00222778,  0.06054688,\n",
      "       -0.04199219,  0.20800781, -0.20214844,  0.01300049,  0.03417969,\n",
      "        0.03027344, -0.09228516, -0.14648438, -0.09619141, -0.02416992,\n",
      "       -0.27929688,  0.17285156, -0.07763672, -0.05102539,  0.15429688,\n",
      "       -0.01733398, -0.01177979, -0.01940918, -0.07128906,  0.02331543,\n",
      "       -0.07177734, -0.1875    ,  0.04638672, -0.05249023, -0.05566406,\n",
      "       -0.2734375 ,  0.11279297,  0.17089844,  0.01782227,  0.00387573,\n",
      "        0.00753784, -0.20996094, -0.25390625,  0.2265625 , -0.04614258,\n",
      "       -0.06738281, -0.02160645,  0.10791016,  0.26757812, -0.18554688,\n",
      "        0.09326172, -0.09521484,  0.35546875,  0.05737305,  0.45898438,\n",
      "       -0.36914062, -0.05517578, -0.14941406,  0.3984375 ,  0.09277344,\n",
      "        0.03393555, -0.06152344,  0.15917969,  0.12792969,  0.00222778],\n",
      "      dtype=float32), array([ 0.00704956, -0.07324219,  0.171875  ,  0.02258301, -0.1328125 ,\n",
      "        0.19824219,  0.11279297, -0.10791016,  0.07177734,  0.02087402,\n",
      "       -0.12304688, -0.05908203,  0.10107422,  0.01074219,  0.14355469,\n",
      "        0.25976562, -0.03637695,  0.18554688, -0.07861328, -0.02270508,\n",
      "       -0.12060547,  0.17773438,  0.04956055,  0.01721191,  0.07958984,\n",
      "       -0.0456543 , -0.18847656,  0.18945312, -0.02319336,  0.06298828,\n",
      "        0.09765625, -0.01904297, -0.07910156,  0.15234375,  0.17382812,\n",
      "        0.1015625 , -0.16308594,  0.11474609,  0.10058594, -0.09277344,\n",
      "        0.109375  ,  0.05883789, -0.02160645,  0.06347656,  0.04199219,\n",
      "       -0.0088501 ,  0.03222656,  0.10644531,  0.06445312, -0.11865234,\n",
      "        0.03051758,  0.06689453,  0.12207031, -0.08300781,  0.171875  ,\n",
      "        0.07861328,  0.09521484, -0.00778198,  0.02319336,  0.0234375 ,\n",
      "       -0.0168457 ,  0.15527344, -0.10986328, -0.17675781, -0.11621094,\n",
      "        0.0234375 , -0.01062012,  0.05273438, -0.13378906,  0.07958984,\n",
      "        0.07373047,  0.04394531,  0.11523438, -0.02062988,  0.07470703,\n",
      "       -0.01153564,  0.08056641,  0.04174805,  0.08007812,  0.3515625 ,\n",
      "        0.09667969, -0.21289062,  0.16503906, -0.078125  ,  0.06982422,\n",
      "       -0.00139618, -0.09130859,  0.12988281,  0.25195312, -0.01611328,\n",
      "        0.09326172, -0.14648438, -0.00151062, -0.15136719, -0.02685547,\n",
      "       -0.15722656,  0.02636719,  0.0859375 ,  0.07177734,  0.07714844,\n",
      "       -0.0390625 ,  0.05444336, -0.12792969,  0.09130859, -0.18457031,\n",
      "       -0.03759766, -0.0279541 , -0.08984375, -0.11669922, -0.09863281,\n",
      "        0.0480957 , -0.16210938, -0.10888672,  0.08496094, -0.0456543 ,\n",
      "        0.15820312, -0.03808594, -0.08203125,  0.203125  ,  0.08642578,\n",
      "        0.06933594,  0.03222656, -0.16015625,  0.09472656, -0.0246582 ,\n",
      "        0.05419922,  0.0279541 ,  0.04492188,  0.16992188,  0.07275391,\n",
      "       -0.03637695, -0.01025391, -0.01708984, -0.10742188, -0.0007019 ,\n",
      "       -0.07373047,  0.25390625,  0.05664062,  0.03515625, -0.00860596,\n",
      "        0.18554688,  0.02148438,  0.26367188, -0.02380371, -0.09912109,\n",
      "       -0.04125977, -0.06933594, -0.11376953,  0.05004883, -0.05883789,\n",
      "        0.04614258,  0.08740234,  0.10546875,  0.10644531,  0.0279541 ,\n",
      "        0.09472656,  0.11621094, -0.17285156, -0.03491211, -0.20800781,\n",
      "        0.05957031,  0.10400391, -0.00179291,  0.05859375, -0.02978516,\n",
      "       -0.03759766,  0.04858398, -0.06396484,  0.07958984,  0.06933594,\n",
      "       -0.10498047, -0.14453125,  0.04345703, -0.06884766, -0.03564453,\n",
      "       -0.01171875,  0.01367188, -0.06591797,  0.11914062,  0.03125   ,\n",
      "       -0.04638672, -0.00196838,  0.00735474, -0.05664062,  0.02783203,\n",
      "        0.08251953, -0.01348877,  0.07177734,  0.14453125,  0.12792969,\n",
      "        0.04223633,  0.14160156, -0.01806641,  0.02160645, -0.09179688,\n",
      "        0.13378906, -0.1953125 , -0.05029297, -0.0378418 , -0.09619141,\n",
      "        0.10302734, -0.10693359, -0.14746094,  0.09960938, -0.23046875,\n",
      "        0.22753906, -0.07519531,  0.06494141,  0.09179688,  0.046875  ,\n",
      "        0.06298828,  0.06982422,  0.04614258,  0.09716797, -0.20214844,\n",
      "        0.19921875,  0.18652344, -0.11962891, -0.14257812,  0.15039062,\n",
      "       -0.03369141, -0.14550781, -0.00069046, -0.07324219,  0.13378906,\n",
      "        0.03564453, -0.02294922,  0.02770996, -0.07910156,  0.20703125,\n",
      "       -0.08349609, -0.04956055,  0.03149414,  0.1484375 ,  0.05566406,\n",
      "       -0.04492188, -0.07958984,  0.00476074, -0.02075195,  0.06005859,\n",
      "        0.00476074,  0.01116943,  0.17285156, -0.13476562,  0.03076172,\n",
      "       -0.07958984,  0.09033203,  0.06103516,  0.07714844, -0.05029297,\n",
      "       -0.09228516, -0.26757812,  0.10791016,  0.0859375 ,  0.06298828,\n",
      "        0.10791016, -0.0267334 ,  0.10205078, -0.12060547,  0.05297852,\n",
      "        0.09472656, -0.16503906,  0.04418945,  0.07226562,  0.04125977,\n",
      "        0.42578125, -0.10302734, -0.16015625, -0.09033203, -0.06396484,\n",
      "       -0.0480957 ,  0.14453125,  0.06542969,  0.04931641,  0.05419922,\n",
      "        0.13574219, -0.01928711, -0.21582031, -0.07421875, -0.14648438,\n",
      "        0.01147461, -0.16503906, -0.10498047,  0.00320435,  0.13476562,\n",
      "       -0.00396729, -0.10351562, -0.13964844,  0.10449219, -0.01257324,\n",
      "       -0.23339844, -0.03637695, -0.09375   ,  0.18261719,  0.02709961,\n",
      "        0.12792969, -0.02478027,  0.01123047,  0.1640625 ,  0.10693359],\n",
      "      dtype=float32), array([ 0.07080078, -0.03491211,  0.06542969,  0.05883789, -0.16601562,\n",
      "        0.21386719,  0.23339844, -0.171875  , -0.04760742, -0.02746582,\n",
      "        0.02368164, -0.36328125, -0.16894531, -0.00375366, -0.30078125,\n",
      "        0.22265625,  0.06396484,  0.08105469,  0.32421875, -0.05615234,\n",
      "       -0.11621094, -0.0625    ,  0.01916504, -0.28320312,  0.13867188,\n",
      "        0.07568359, -0.05932617,  0.00148773, -0.00221252, -0.03112793,\n",
      "       -0.14746094,  0.16699219, -0.02661133, -0.20996094,  0.09179688,\n",
      "        0.10009766,  0.21875   ,  0.02929688, -0.02880859, -0.05834961,\n",
      "       -0.04394531, -0.13183594,  0.20019531, -0.03637695, -0.00421143,\n",
      "        0.02490234, -0.13378906,  0.14648438, -0.04370117,  0.06591797,\n",
      "        0.03112793,  0.13476562, -0.0625    , -0.04785156,  0.03198242,\n",
      "        0.07666016, -0.04125977, -0.27734375,  0.0027771 , -0.17382812,\n",
      "        0.07080078,  0.00384521, -0.02209473, -0.04980469, -0.00674438,\n",
      "        0.05517578, -0.01055908, -0.00405884, -0.15917969,  0.15917969,\n",
      "        0.03613281, -0.03686523,  0.00665283, -0.10888672, -0.34570312,\n",
      "       -0.24121094,  0.03051758, -0.05273438,  0.3125    , -0.0098877 ,\n",
      "       -0.00216675, -0.05371094,  0.12158203,  0.01470947, -0.2890625 ,\n",
      "       -0.12402344, -0.00466919,  0.22070312, -0.01696777, -0.15332031,\n",
      "        0.11621094,  0.05688477, -0.24316406,  0.02307129, -0.00683594,\n",
      "       -0.24511719,  0.24902344,  0.26953125, -0.03710938,  0.0145874 ,\n",
      "       -0.15917969,  0.12695312,  0.09960938,  0.17773438, -0.16699219,\n",
      "       -0.08398438, -0.06640625, -0.02624512,  0.04174805, -0.24804688,\n",
      "       -0.00308228, -0.11572266,  0.05566406,  0.02294922,  0.06689453,\n",
      "       -0.03027344,  0.16015625, -0.14648438, -0.18261719,  0.01123047,\n",
      "       -0.08300781,  0.1875    ,  0.20214844,  0.03393555, -0.12597656,\n",
      "       -0.04125977, -0.04858398,  0.04736328,  0.00196838, -0.23730469,\n",
      "       -0.1796875 , -0.17382812,  0.00105286,  0.06835938,  0.02844238,\n",
      "       -0.203125  ,  0.04125977,  0.07373047,  0.03393555,  0.22070312,\n",
      "       -0.0246582 , -0.28710938,  0.08984375, -0.00921631,  0.05932617,\n",
      "       -0.04980469, -0.0456543 , -0.11669922, -0.06347656, -0.08886719,\n",
      "        0.18457031,  0.03369141, -0.26171875, -0.06738281, -0.02050781,\n",
      "        0.10302734, -0.17871094, -0.08544922, -0.02282715,  0.0480957 ,\n",
      "        0.12109375,  0.25      ,  0.05078125,  0.16601562,  0.01177979,\n",
      "       -0.00701904,  0.12060547, -0.08300781,  0.01672363,  0.0612793 ,\n",
      "       -0.05151367,  0.06689453,  0.06738281, -0.21484375, -0.18359375,\n",
      "       -0.09277344,  0.21679688, -0.13867188,  0.0100708 , -0.12451172,\n",
      "       -0.18945312, -0.0625    ,  0.22265625, -0.02746582,  0.12988281,\n",
      "       -0.01831055, -0.07763672, -0.02185059,  0.01818848,  0.16503906,\n",
      "        0.18457031,  0.08056641,  0.12207031,  0.12353516,  0.24511719,\n",
      "       -0.0559082 , -0.01843262,  0.15820312,  0.03393555, -0.125     ,\n",
      "       -0.15625   ,  0.08007812, -0.10888672,  0.24707031,  0.0625    ,\n",
      "        0.13085938,  0.00585938, -0.265625  , -0.05810547,  0.04052734,\n",
      "       -0.02368164,  0.03540039, -0.09179688, -0.07958984, -0.33398438,\n",
      "        0.04443359,  0.20117188, -0.11279297, -0.1875    , -0.203125  ,\n",
      "       -0.11376953, -0.02905273, -0.16113281,  0.0145874 ,  0.11621094,\n",
      "       -0.00714111,  0.09863281,  0.13378906, -0.03015137,  0.04370117,\n",
      "        0.05444336,  0.11474609, -0.00946045, -0.05102539,  0.12792969,\n",
      "       -0.13769531, -0.09179688, -0.22070312,  0.29882812, -0.125     ,\n",
      "        0.13085938, -0.09179688,  0.17578125, -0.09863281,  0.00073242,\n",
      "        0.08544922, -0.09033203, -0.10351562,  0.14746094,  0.01757812,\n",
      "        0.04760742,  0.06982422, -0.04467773,  0.19433594,  0.22167969,\n",
      "       -0.03466797, -0.04345703,  0.03173828,  0.07568359, -0.07226562,\n",
      "        0.01782227,  0.08789062, -0.24414062, -0.02050781, -0.19042969,\n",
      "        0.26171875, -0.00117493, -0.01367188, -0.22167969, -0.03393555,\n",
      "        0.09277344,  0.28515625,  0.35351562,  0.125     , -0.19628906,\n",
      "       -0.09472656, -0.07568359, -0.06591797,  0.12109375, -0.17773438,\n",
      "        0.08642578,  0.10742188,  0.03735352,  0.3125    ,  0.06005859,\n",
      "        0.16308594,  0.02807617, -0.38085938, -0.13964844, -0.01177979,\n",
      "       -0.22558594,  0.11621094,  0.01953125,  0.17871094, -0.07861328,\n",
      "       -0.10595703,  0.20019531, -0.20800781, -0.09326172, -0.171875  ],\n",
      "      dtype=float32), array([ 0.07910156, -0.0050354 ,  0.11181641,  0.21289062,  0.13085938,\n",
      "       -0.01470947, -0.03540039, -0.07763672,  0.04077148,  0.11474609,\n",
      "        0.00147247, -0.29101562,  0.00457764, -0.20019531, -0.19238281,\n",
      "        0.08007812,  0.10107422,  0.04858398,  0.15722656, -0.09521484,\n",
      "       -0.05004883,  0.25      ,  0.33007812, -0.09716797, -0.05566406,\n",
      "       -0.0071106 , -0.16796875, -0.13574219,  0.05102539, -0.00598145,\n",
      "        0.10791016,  0.16503906, -0.03955078, -0.03955078,  0.04321289,\n",
      "        0.12060547,  0.13476562,  0.09375   ,  0.00909424,  0.1640625 ,\n",
      "        0.21289062, -0.05322266,  0.33398438,  0.01586914,  0.10449219,\n",
      "        0.24121094, -0.0189209 , -0.04199219,  0.05834961,  0.03271484,\n",
      "        0.09863281,  0.18945312,  0.04125977,  0.01501465, -0.05883789,\n",
      "        0.10253906,  0.01538086,  0.03198242,  0.02722168, -0.13769531,\n",
      "        0.12695312,  0.06396484, -0.13574219, -0.012146  ,  0.07617188,\n",
      "       -0.02319336, -0.21191406,  0.20996094, -0.01953125,  0.02038574,\n",
      "        0.16113281, -0.00897217,  0.04663086,  0.03881836, -0.4609375 ,\n",
      "       -0.1796875 ,  0.12792969, -0.00564575,  0.24121094,  0.21777344,\n",
      "       -0.02600098, -0.1171875 ,  0.19140625, -0.04052734, -0.18261719,\n",
      "       -0.09619141, -0.05053711,  0.27734375,  0.00086975,  0.06298828,\n",
      "       -0.13085938,  0.1328125 , -0.06640625,  0.03491211, -0.26953125,\n",
      "        0.0267334 ,  0.11816406,  0.1328125 , -0.02490234, -0.17480469,\n",
      "       -0.234375  , -0.0703125 ,  0.09179688,  0.03198242, -0.10791016,\n",
      "        0.13671875, -0.11523438,  0.00732422,  0.04223633, -0.25585938,\n",
      "       -0.27734375, -0.10644531,  0.02612305,  0.09082031,  0.14355469,\n",
      "       -0.19335938, -0.07421875, -0.20898438,  0.21289062, -0.17382812,\n",
      "       -0.04980469, -0.10839844, -0.02038574,  0.22949219, -0.09570312,\n",
      "       -0.06103516, -0.10693359,  0.03369141,  0.01879883,  0.05029297,\n",
      "       -0.36132812, -0.24511719, -0.34765625, -0.04394531, -0.18261719,\n",
      "       -0.14453125, -0.06152344,  0.21386719,  0.09667969,  0.1796875 ,\n",
      "       -0.02160645,  0.03100586,  0.0703125 , -0.07275391,  0.07226562,\n",
      "        0.10644531, -0.25      , -0.20800781, -0.0378418 , -0.01446533,\n",
      "        0.171875  ,  0.17578125, -0.33789062,  0.05517578, -0.01965332,\n",
      "        0.13378906, -0.01647949, -0.04541016, -0.16796875,  0.08544922,\n",
      "        0.09960938, -0.02001953, -0.06347656,  0.32421875, -0.02404785,\n",
      "       -0.22949219,  0.09960938,  0.08886719,  0.13769531, -0.05517578,\n",
      "       -0.2109375 ,  0.1875    , -0.03833008, -0.16796875, -0.11132812,\n",
      "        0.05249023,  0.1875    , -0.15136719,  0.07275391, -0.01953125,\n",
      "       -0.06933594,  0.16894531,  0.1171875 ,  0.10351562,  0.02905273,\n",
      "        0.0546875 , -0.2578125 ,  0.1171875 ,  0.0703125 , -0.04711914,\n",
      "       -0.03710938,  0.16503906, -0.01159668, -0.04638672,  0.11621094,\n",
      "        0.09521484, -0.09130859,  0.01916504, -0.08886719, -0.20507812,\n",
      "       -0.13183594,  0.12060547,  0.04980469,  0.09863281,  0.02819824,\n",
      "        0.046875  , -0.18554688, -0.06347656,  0.07519531,  0.11767578,\n",
      "        0.08154297,  0.0144043 , -0.11132812,  0.00939941, -0.23828125,\n",
      "       -0.1796875 ,  0.265625  ,  0.04174805, -0.0859375 , -0.06298828,\n",
      "       -0.1953125 ,  0.03588867, -0.03295898,  0.04174805,  0.23339844,\n",
      "       -0.3515625 ,  0.16503906, -0.0625    , -0.15136719, -0.10009766,\n",
      "        0.03979492, -0.01342773,  0.0123291 , -0.00939941,  0.12109375,\n",
      "        0.06738281, -0.18457031,  0.15917969,  0.12109375, -0.1171875 ,\n",
      "        0.00302124,  0.11425781,  0.11767578, -0.07666016, -0.16503906,\n",
      "       -0.09570312, -0.09277344,  0.12792969, -0.09375   ,  0.04223633,\n",
      "        0.21679688, -0.0859375 , -0.05883789,  0.16015625,  0.03710938,\n",
      "        0.01708984,  0.00326538, -0.13183594, -0.17480469, -0.27539062,\n",
      "       -0.03417969, -0.10546875,  0.07226562, -0.05541992,  0.09228516,\n",
      "        0.18164062,  0.04003906,  0.07275391, -0.14355469,  0.13378906,\n",
      "       -0.05371094,  0.15234375,  0.265625  ,  0.24609375, -0.05419922,\n",
      "       -0.07177734, -0.08496094, -0.25585938, -0.08300781, -0.03271484,\n",
      "        0.08203125,  0.00077057,  0.0480957 ,  0.17089844,  0.13964844,\n",
      "        0.03588867, -0.06542969, -0.2734375 , -0.22753906,  0.09570312,\n",
      "       -0.12597656,  0.15332031, -0.30664062, -0.07861328, -0.08642578,\n",
      "       -0.11474609, -0.02929688, -0.0067749 ,  0.04272461, -0.10351562],\n",
      "      dtype=float32), array([ 5.17578125e-02, -1.33056641e-02,  1.78710938e-01,  2.59765625e-01,\n",
      "       -1.39648438e-01, -5.51757812e-02,  1.03515625e-01, -2.85644531e-02,\n",
      "        1.12304688e-01,  3.41796875e-02, -1.97265625e-01, -2.21679688e-01,\n",
      "       -6.34765625e-02,  5.34667969e-02, -3.28063965e-03,  2.16796875e-01,\n",
      "        2.51953125e-01,  1.13281250e-01,  1.15722656e-01,  3.17382812e-02,\n",
      "       -3.35937500e-01, -4.15039062e-02,  1.59179688e-01,  5.05371094e-02,\n",
      "        5.02929688e-02,  5.22460938e-02, -1.48437500e-01,  8.39843750e-02,\n",
      "       -1.37695312e-01,  1.54296875e-01, -3.46679688e-02,  1.68945312e-01,\n",
      "       -1.04980469e-01, -8.59375000e-02, -2.81250000e-01,  1.89453125e-01,\n",
      "        1.38671875e-01,  2.62451172e-02,  1.22070312e-01,  1.19628906e-01,\n",
      "        7.61718750e-02, -2.31445312e-01,  2.25585938e-01, -3.20312500e-01,\n",
      "       -1.95312500e-02, -4.60815430e-03, -1.79443359e-02, -2.50000000e-01,\n",
      "        1.41601562e-01,  4.71191406e-02, -2.28515625e-01, -3.32031250e-02,\n",
      "       -1.93359375e-01,  1.64062500e-01,  7.95898438e-02,  3.30078125e-01,\n",
      "        1.48925781e-02,  4.88281250e-02,  1.75781250e-01, -1.90429688e-01,\n",
      "        2.77343750e-01,  3.80859375e-02, -4.05273438e-02, -3.16406250e-01,\n",
      "       -1.58203125e-01, -2.71484375e-01, -1.00585938e-01,  1.49414062e-01,\n",
      "       -4.41894531e-02, -1.38671875e-01,  1.74804688e-01,  1.59179688e-01,\n",
      "        5.95703125e-02,  2.55859375e-01, -1.99218750e-01, -2.06298828e-02,\n",
      "        1.35742188e-01,  1.14257812e-01,  8.25195312e-02, -1.47094727e-02,\n",
      "       -2.89062500e-01,  4.32128906e-02, -2.27050781e-02,  1.56250000e-01,\n",
      "       -1.25000000e-01,  6.13403320e-03,  1.32812500e-01,  5.32226562e-02,\n",
      "       -1.31835938e-01, -6.49414062e-02, -8.34960938e-02,  2.06298828e-02,\n",
      "       -2.53906250e-01, -3.11279297e-02, -5.46875000e-02, -4.24194336e-03,\n",
      "       -5.12695312e-02,  7.03125000e-02, -5.12695312e-02,  1.59179688e-01,\n",
      "       -2.65625000e-01,  5.54199219e-02, -7.76367188e-02, -6.68945312e-02,\n",
      "       -5.51757812e-02, -2.11914062e-01, -9.96093750e-02, -6.12792969e-02,\n",
      "        1.20117188e-01, -8.00781250e-02, -3.12500000e-01, -5.05371094e-02,\n",
      "        4.14848328e-05,  1.83593750e-01,  3.49609375e-01, -4.73632812e-02,\n",
      "        1.53198242e-02,  1.03027344e-01,  1.92871094e-02,  5.61523438e-02,\n",
      "       -3.00781250e-01, -1.00585938e-01,  3.44238281e-02, -2.08007812e-01,\n",
      "        1.88476562e-01, -5.49316406e-02,  7.12890625e-02,  1.31835938e-02,\n",
      "       -2.68554688e-02, -7.76367188e-02, -2.61718750e-01, -9.81445312e-02,\n",
      "        4.32128906e-02,  8.93554688e-02,  6.64062500e-02,  7.81250000e-02,\n",
      "        1.12792969e-01, -6.44531250e-02, -1.08886719e-01,  5.44433594e-02,\n",
      "       -9.15527344e-03, -2.94921875e-01,  1.16210938e-01, -7.42187500e-02,\n",
      "        5.81054688e-02, -5.03540039e-03, -3.49121094e-02, -1.23535156e-01,\n",
      "       -1.28906250e-01,  5.03540039e-03,  9.61914062e-02,  3.71093750e-02,\n",
      "       -1.42578125e-01,  2.59765625e-01, -5.46875000e-02, -9.03320312e-02,\n",
      "       -6.73828125e-02,  1.76757812e-01, -2.06054688e-01,  3.07617188e-02,\n",
      "       -6.20117188e-02, -1.35742188e-01, -2.71484375e-01,  2.28515625e-01,\n",
      "        6.83593750e-02, -7.32421875e-03,  1.12792969e-01,  1.27929688e-01,\n",
      "       -1.99218750e-01,  3.96728516e-03, -4.49218750e-02,  1.96533203e-02,\n",
      "       -2.47192383e-03, -9.32617188e-02, -1.37695312e-01,  1.60156250e-01,\n",
      "        1.95312500e-01, -6.44531250e-02,  7.86132812e-02, -1.74560547e-02,\n",
      "        9.52148438e-02,  3.46679688e-02,  7.32421875e-02,  1.95312500e-01,\n",
      "        1.57226562e-01,  2.02636719e-02, -1.05468750e-01, -1.23535156e-01,\n",
      "       -3.39355469e-02,  6.98242188e-02,  1.66015625e-01,  2.14843750e-02,\n",
      "        2.31445312e-01,  2.35595703e-02,  2.27539062e-01,  1.25976562e-01,\n",
      "        1.01470947e-03, -2.15820312e-01, -2.16064453e-02, -8.88671875e-02,\n",
      "       -1.64794922e-02,  2.57568359e-02, -3.54003906e-02,  1.84570312e-01,\n",
      "        3.59375000e-01, -1.27929688e-01, -5.81054688e-02, -2.57568359e-02,\n",
      "        4.05883789e-03,  9.42382812e-02,  3.85742188e-02,  3.66210938e-02,\n",
      "       -2.77099609e-02,  1.06445312e-01, -1.69921875e-01, -1.11328125e-01,\n",
      "        1.63085938e-01,  4.83398438e-02, -8.69140625e-02, -1.08886719e-01,\n",
      "       -2.04101562e-01,  8.30078125e-02, -1.42578125e-01,  3.29589844e-02,\n",
      "        8.74023438e-02, -1.80664062e-01,  1.96289062e-01,  2.04101562e-01,\n",
      "       -1.10351562e-01, -2.80761719e-02,  1.40625000e-01, -4.27246094e-02,\n",
      "       -3.90625000e-03, -8.83789062e-02,  1.83593750e-01, -6.73828125e-02,\n",
      "        8.54492188e-02, -5.24902344e-02,  2.46093750e-01, -7.62939453e-03,\n",
      "        1.46484375e-01, -3.71093750e-02,  3.83300781e-02, -1.45507812e-01,\n",
      "       -3.28125000e-01,  5.54199219e-02,  1.92382812e-01,  2.21679688e-01,\n",
      "        5.66406250e-02, -6.39648438e-02, -1.96838379e-03, -3.90625000e-03,\n",
      "        4.27734375e-01, -9.82666016e-03, -1.74560547e-02, -2.35595703e-02,\n",
      "        1.69921875e-01,  3.29589844e-02, -1.47460938e-01, -2.00195312e-01,\n",
      "        1.16699219e-01,  1.90429688e-02, -3.71093750e-02, -2.20947266e-02,\n",
      "       -1.07910156e-01,  1.04003906e-01, -2.29492188e-01, -1.12304688e-02,\n",
      "        2.66113281e-02, -1.86767578e-02,  4.29687500e-02, -4.49218750e-02,\n",
      "        2.43164062e-01,  5.68847656e-02, -8.11767578e-03,  8.30078125e-02,\n",
      "        4.85839844e-02, -1.08032227e-02, -1.36718750e-01,  1.29882812e-01,\n",
      "        6.20117188e-02, -1.05468750e-01, -7.61718750e-02,  1.08398438e-01,\n",
      "        3.06396484e-02, -1.64794922e-02,  1.02539062e-01, -2.20703125e-01,\n",
      "       -1.22070312e-02, -5.56640625e-02,  2.05078125e-01, -2.08984375e-01,\n",
      "       -2.38281250e-01,  1.03027344e-01,  5.93261719e-02, -1.30859375e-01,\n",
      "        2.45361328e-02, -3.16406250e-01, -8.88671875e-02,  1.31835938e-01],\n",
      "      dtype=float32), array([-1.39648438e-01, -3.46679688e-02, -5.37109375e-02,  1.79687500e-01,\n",
      "       -3.68652344e-02, -2.57568359e-02,  4.85229492e-03, -8.34960938e-02,\n",
      "        8.17871094e-03,  3.24218750e-01, -9.08203125e-02, -1.47460938e-01,\n",
      "       -1.08398438e-01,  5.73730469e-02, -1.00708008e-02, -2.38037109e-03,\n",
      "        2.07031250e-01,  8.98437500e-02, -1.61132812e-01, -3.93066406e-02,\n",
      "       -1.03027344e-01, -7.12890625e-02,  1.56250000e-01, -2.29492188e-01,\n",
      "        1.27929688e-01,  7.42187500e-02, -1.12304688e-01, -1.13769531e-01,\n",
      "        3.22265625e-02, -1.26342773e-02, -4.66918945e-03,  7.47070312e-02,\n",
      "       -1.14746094e-01,  3.78417969e-02,  1.80664062e-01, -3.39355469e-02,\n",
      "        1.33789062e-01, -1.87500000e-01, -5.54199219e-02,  1.25976562e-01,\n",
      "        1.50390625e-01,  1.87500000e-01, -6.02722168e-04, -4.83398438e-02,\n",
      "       -4.54101562e-02, -1.31835938e-01, -1.64794922e-02,  3.19824219e-02,\n",
      "        1.18164062e-01, -6.39648438e-02, -5.27343750e-02, -5.73730469e-03,\n",
      "       -1.46484375e-01, -5.88378906e-02, -3.97949219e-02,  5.41992188e-02,\n",
      "        3.01513672e-02, -4.51660156e-02,  5.44433594e-02, -1.21093750e-01,\n",
      "       -3.85742188e-02,  1.54296875e-01, -1.84326172e-02,  1.11083984e-02,\n",
      "        1.86767578e-02, -9.81445312e-02, -2.47802734e-02,  1.52343750e-01,\n",
      "       -7.08007812e-02,  5.20019531e-02, -2.44140625e-02,  1.25000000e-01,\n",
      "        2.28881836e-03,  1.19628906e-01, -1.02539062e-01, -2.38037109e-02,\n",
      "        1.82617188e-01,  1.88476562e-01, -5.22460938e-02,  7.47070312e-02,\n",
      "        5.68847656e-02, -5.05371094e-02,  8.39843750e-02,  4.68750000e-02,\n",
      "       -4.83398438e-02, -6.88476562e-02, -2.67578125e-01,  2.27050781e-02,\n",
      "       -1.58203125e-01, -1.01928711e-02, -9.94873047e-03,  1.25976562e-01,\n",
      "       -5.81054688e-02, -1.33789062e-01, -2.09960938e-02, -1.09863281e-01,\n",
      "        3.75366211e-03,  8.39843750e-02, -6.59179688e-02, -3.06396484e-02,\n",
      "        1.66015625e-01, -2.40234375e-01,  6.54296875e-02,  3.80859375e-02,\n",
      "       -5.22460938e-02, -5.34667969e-02, -1.13769531e-01, -9.37500000e-02,\n",
      "        1.76757812e-01, -6.88476562e-02, -2.02148438e-01,  7.56835938e-02,\n",
      "       -4.51660156e-02, -8.05664062e-02,  4.68750000e-02, -5.05371094e-02,\n",
      "        9.91210938e-02, -8.49609375e-02,  2.63671875e-02, -2.11181641e-02,\n",
      "       -9.32617188e-02, -3.88183594e-02, -9.47265625e-02,  1.25976562e-01,\n",
      "       -2.01171875e-01, -1.45507812e-01, -1.92871094e-02, -1.12304688e-01,\n",
      "        2.13623047e-02, -7.71484375e-02,  1.12304688e-01, -1.45507812e-01,\n",
      "       -2.10937500e-01, -1.11816406e-01,  2.38037109e-03, -1.07910156e-01,\n",
      "        1.23535156e-01, -2.00195312e-02, -5.02929688e-02, -1.08642578e-02,\n",
      "       -4.49218750e-02, -4.22363281e-02,  3.24707031e-02,  1.77001953e-03,\n",
      "        2.66113281e-02,  2.27050781e-02, -2.90527344e-02, -1.17187500e-01,\n",
      "       -2.09960938e-01,  1.53198242e-02, -8.39843750e-02, -9.22851562e-02,\n",
      "       -2.23632812e-01, -9.13085938e-02,  7.03125000e-02,  8.66699219e-03,\n",
      "       -1.46484375e-01, -1.49414062e-01, -1.42211914e-02,  8.69140625e-02,\n",
      "        5.37109375e-02,  1.00585938e-01, -4.80957031e-02,  9.37500000e-02,\n",
      "        8.83789062e-02, -1.54296875e-01, -9.52148438e-02, -1.66992188e-01,\n",
      "       -1.56402588e-03, -9.57031250e-02, -1.77734375e-01, -4.29687500e-02,\n",
      "       -2.57812500e-01, -7.76367188e-02, -2.08740234e-02, -1.83593750e-01,\n",
      "       -8.49609375e-02, -9.32617188e-02,  5.51757812e-02, -6.20117188e-02,\n",
      "       -4.46777344e-02, -1.37695312e-01,  1.49414062e-01,  9.96093750e-02,\n",
      "        3.56445312e-02, -2.14843750e-01, -2.60009766e-02, -6.22558594e-02,\n",
      "        1.15722656e-01,  1.20605469e-01,  7.69042969e-03,  5.27343750e-02,\n",
      "        2.89306641e-02, -2.84423828e-02, -4.07714844e-02, -1.17187500e-01,\n",
      "        1.25976562e-01,  3.31115723e-03,  3.39355469e-02, -1.83593750e-01,\n",
      "        1.13769531e-01,  1.53320312e-01, -7.17773438e-02, -6.43920898e-03,\n",
      "       -1.97753906e-02, -1.73828125e-01, -6.59179688e-02, -1.24511719e-01,\n",
      "        8.88671875e-02,  2.22167969e-02, -4.10156250e-02, -8.44726562e-02,\n",
      "        6.07910156e-02,  9.86328125e-02, -1.31835938e-01,  1.01318359e-02,\n",
      "        1.09863281e-01, -8.20312500e-02, -1.08398438e-01, -4.98046875e-02,\n",
      "       -6.68945312e-02, -1.95312500e-02, -3.71093750e-02, -3.10058594e-02,\n",
      "        3.97949219e-02, -9.52148438e-03,  1.15722656e-01,  2.83203125e-02,\n",
      "        3.24707031e-02, -3.83300781e-02,  1.04492188e-01, -1.62109375e-01,\n",
      "        5.27343750e-02,  6.22558594e-02,  9.96093750e-02,  1.94091797e-02,\n",
      "       -5.56640625e-02, -1.09375000e-01,  8.69140625e-02, -1.54296875e-01,\n",
      "       -9.37500000e-02,  8.00781250e-02,  1.25000000e-01,  2.72750854e-04,\n",
      "        9.76562500e-02,  1.42578125e-01,  7.08007812e-02,  7.81250000e-02,\n",
      "       -1.44531250e-01,  2.29492188e-02, -3.68652344e-02,  9.13085938e-02,\n",
      "        1.14257812e-01,  4.93164062e-02, -2.19726562e-02, -1.05590820e-02,\n",
      "        3.88183594e-02,  2.55859375e-01, -7.66601562e-02, -1.27929688e-01,\n",
      "        7.17773438e-02, -3.61328125e-02, -1.22680664e-02,  2.16796875e-01,\n",
      "        4.17480469e-02,  2.44140625e-01, -8.25195312e-02,  1.42578125e-01,\n",
      "        8.34960938e-02,  3.29589844e-03,  2.15820312e-01,  2.29492188e-01,\n",
      "        1.30859375e-01,  7.81250000e-02, -8.83789062e-02, -1.97753906e-02,\n",
      "        1.32446289e-02, -1.92382812e-01,  4.58984375e-02, -2.22656250e-01,\n",
      "       -1.80664062e-02,  4.93164062e-02,  1.62109375e-01,  6.83593750e-02,\n",
      "        5.05371094e-02,  6.14166260e-04, -7.37304688e-02,  2.08740234e-02,\n",
      "        3.75976562e-02,  1.78710938e-01, -4.90722656e-02,  6.34765625e-02,\n",
      "        8.05664062e-02,  8.88671875e-02,  7.76367188e-02, -4.66308594e-02,\n",
      "       -7.17773438e-02, -1.18164062e-01, -2.45666504e-03, -7.22656250e-02],\n",
      "      dtype=float32), 0, array([-2.32421875e-01,  9.03320312e-02,  7.81250000e-02,  1.26953125e-01,\n",
      "        2.97851562e-02,  3.71093750e-01,  5.56640625e-02, -9.42382812e-02,\n",
      "       -2.61718750e-01,  3.68652344e-02,  8.44726562e-02, -1.88476562e-01,\n",
      "        6.05468750e-02, -4.94384766e-03, -1.05957031e-01,  3.10546875e-01,\n",
      "        5.66406250e-02, -1.41601562e-01,  3.49121094e-02, -7.03125000e-02,\n",
      "        1.28906250e-01,  7.71484375e-02,  3.75000000e-01,  3.05175781e-02,\n",
      "        6.34765625e-02, -1.39648438e-01,  1.18255615e-04, -1.09375000e-01,\n",
      "       -2.96875000e-01, -2.97851562e-02, -1.07910156e-01,  1.29882812e-01,\n",
      "       -2.36328125e-01, -2.37304688e-01, -8.64257812e-02, -1.50756836e-02,\n",
      "        4.27734375e-01, -1.25976562e-01, -2.18750000e-01,  1.11816406e-01,\n",
      "       -1.23901367e-02, -2.39257812e-01,  2.69531250e-01,  5.83496094e-02,\n",
      "        4.56542969e-02, -2.73437500e-01, -4.27734375e-01,  3.10058594e-02,\n",
      "       -1.24511719e-01,  1.04980469e-01, -7.37304688e-02, -1.29882812e-01,\n",
      "        1.05468750e-01, -8.93554688e-02, -4.85839844e-02, -3.18359375e-01,\n",
      "        2.98828125e-01, -7.95898438e-02, -2.17773438e-01,  9.21630859e-03,\n",
      "       -2.19726562e-02,  1.51367188e-01, -3.28125000e-01,  1.55273438e-01,\n",
      "        2.05078125e-01, -6.29882812e-02, -1.55273438e-01, -1.20117188e-01,\n",
      "        7.66601562e-02,  5.46875000e-02,  3.24218750e-01,  1.35742188e-01,\n",
      "       -6.28662109e-03,  1.31835938e-01,  1.13281250e-01, -2.55859375e-01,\n",
      "       -3.55468750e-01,  7.32421875e-02, -1.44531250e-01, -1.49414062e-01,\n",
      "        2.34375000e-01, -5.41992188e-02,  9.66796875e-02,  1.20117188e-01,\n",
      "       -2.96630859e-02,  6.73828125e-02, -2.39257812e-01,  4.96093750e-01,\n",
      "       -1.54296875e-01, -9.76562500e-02, -1.28784180e-02, -2.15820312e-01,\n",
      "       -4.02343750e-01, -1.15966797e-02,  1.54296875e-01, -2.05078125e-01,\n",
      "        4.68750000e-02, -3.12500000e-01,  1.60156250e-01, -1.13281250e-01,\n",
      "       -3.26171875e-01, -1.10839844e-01,  1.68945312e-01,  2.56347656e-03,\n",
      "        1.29394531e-02, -3.59375000e-01,  2.45117188e-01, -1.24023438e-01,\n",
      "        2.14843750e-01,  8.39843750e-02, -6.78710938e-02, -3.00781250e-01,\n",
      "       -1.40625000e-01, -1.92382812e-01,  4.42504883e-03,  9.08203125e-02,\n",
      "       -3.54003906e-02, -2.35351562e-01, -1.62109375e-01, -9.57031250e-02,\n",
      "        2.79541016e-02,  3.56445312e-02,  1.21093750e-01,  2.11914062e-01,\n",
      "        6.12792969e-02,  1.50390625e-01, -2.75390625e-01,  1.13281250e-01,\n",
      "       -9.91210938e-02,  1.43554688e-01, -7.86132812e-02,  2.46093750e-01,\n",
      "        1.55273438e-01,  3.12500000e-01, -8.05664062e-02,  2.10937500e-01,\n",
      "       -7.61718750e-02, -2.28515625e-01,  1.54296875e-01,  9.13085938e-02,\n",
      "        3.29589844e-02, -2.26562500e-01,  4.78515625e-02, -1.93359375e-01,\n",
      "       -1.54296875e-01,  6.25000000e-02, -3.49609375e-01, -6.73828125e-02,\n",
      "       -1.38671875e-01, -2.57812500e-01, -9.66796875e-02, -4.76074219e-02,\n",
      "       -1.06933594e-01,  2.96875000e-01, -2.10937500e-01, -9.42382812e-02,\n",
      "       -1.16699219e-01, -2.23632812e-01, -4.43359375e-01, -1.22558594e-01,\n",
      "        5.59082031e-02,  1.78710938e-01, -9.42382812e-02,  7.71484375e-02,\n",
      "        1.91406250e-01,  7.91015625e-02,  6.34765625e-02,  2.47192383e-03,\n",
      "        5.82031250e-01, -1.60156250e-01, -3.37890625e-01,  8.74023438e-02,\n",
      "        1.34277344e-02,  9.86328125e-02, -1.49414062e-01,  4.27246094e-02,\n",
      "        1.96289062e-01,  9.32617188e-02,  1.00097656e-01,  4.82177734e-03,\n",
      "       -9.61914062e-02, -1.56250000e-01,  1.05957031e-01, -3.12500000e-01,\n",
      "       -2.33154297e-02, -2.08984375e-01, -1.33789062e-01, -1.63085938e-01,\n",
      "        6.83593750e-03,  8.93554688e-02,  4.27246094e-02,  1.74560547e-02,\n",
      "        1.86523438e-01,  8.25195312e-02,  4.51660156e-02,  3.37890625e-01,\n",
      "        8.83789062e-02,  7.35473633e-03, -1.98242188e-01,  1.70898438e-02,\n",
      "       -2.24609375e-01, -2.33154297e-02, -1.88476562e-01, -4.05273438e-02,\n",
      "       -1.96289062e-01, -1.87500000e-01, -2.59765625e-01,  1.53198242e-02,\n",
      "        7.59887695e-03, -2.37304688e-01, -2.34375000e-01,  6.15234375e-02,\n",
      "        7.71484375e-02,  3.97949219e-02,  1.72851562e-01,  1.91406250e-01,\n",
      "       -1.71875000e-01,  2.34375000e-01, -4.63867188e-02, -6.93359375e-02,\n",
      "        1.60156250e-01,  1.56250000e-01,  3.33984375e-01, -5.41992188e-02,\n",
      "        8.20312500e-02,  8.15429688e-02,  2.77343750e-01, -1.49536133e-02,\n",
      "        1.11694336e-02, -1.76757812e-01,  1.67968750e-01, -2.51953125e-01,\n",
      "        6.00585938e-02,  4.46777344e-02,  2.28515625e-01, -4.80468750e-01,\n",
      "        4.29687500e-01,  2.16674805e-03, -3.49609375e-01, -7.37304688e-02,\n",
      "       -4.19921875e-02,  8.05664062e-02,  7.51953125e-02, -7.11059570e-03,\n",
      "       -2.61718750e-01, -1.42578125e-01, -5.83496094e-02, -9.96093750e-02,\n",
      "        1.59179688e-01, -2.32421875e-01, -4.22363281e-02,  1.25976562e-01,\n",
      "       -3.17382812e-02,  1.05957031e-01,  3.92578125e-01,  2.50244141e-02,\n",
      "       -1.66992188e-01, -1.46484375e-01, -3.24707031e-02, -4.02832031e-02,\n",
      "        1.16210938e-01, -7.91015625e-02, -3.59375000e-01, -3.22265625e-01,\n",
      "       -3.66210938e-02,  7.86132812e-02, -1.37695312e-01,  6.88476562e-02,\n",
      "        4.07714844e-02, -9.13085938e-02,  2.35351562e-01,  9.94873047e-03,\n",
      "        8.59375000e-02,  3.90625000e-02,  1.77734375e-01, -3.28125000e-01,\n",
      "       -4.71191406e-02, -8.93554688e-02, -2.08007812e-01, -3.78906250e-01,\n",
      "        2.57812500e-01, -2.96630859e-02,  7.81250000e-02,  2.01171875e-01,\n",
      "        3.97949219e-02, -1.55273438e-01, -2.03125000e-01,  3.14453125e-01,\n",
      "       -3.04687500e-01,  5.66406250e-01,  2.05078125e-02,  1.18164062e-01,\n",
      "       -5.11718750e-01,  2.94921875e-01, -8.25195312e-02, -1.81640625e-01,\n",
      "        1.52587891e-02, -2.09960938e-01,  2.13867188e-01,  1.70898438e-01],\n",
      "      dtype=float32), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "sentence = data_train[0]['text']\n",
    "print(sentence)\n",
    "embed_ls = [0]*32\n",
    "for i, each in enumerate(sentence.split()):\n",
    "    if each in wv:\n",
    "        embed_ls[i] = wv[each]\n",
    "    else:\n",
    "        print(f'{each} not in wv')\n",
    "print(embed_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(wv['hello'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units, num_layers, output_size):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        \n",
    "        # 定义 LSTM 层\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_units, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True)\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(hidden_units, output_size)\n",
    "\n",
    "    def forward(self, x): # x: (batch_size, seq_len, features)\n",
    "        # 将输入通过 LSTM 层\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # 取最后一个时间步的输出作为全连接层的输入\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class lstm_model_dataset(Dataset):\n",
    "    def __init__(self, data) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index) -> any:\n",
    "        embed_ls = [0]*32 # SEQ_LEN = 32\n",
    "        for i, each in enumerate(self.data[index]['text'].split()):\n",
    "            if i == 32:\n",
    "                break\n",
    "            if each in wv:\n",
    "                embed_ls[i] = wv[each]\n",
    "            else:\n",
    "                pass\n",
    "        for i, each in enumerate(embed_ls):\n",
    "            if type(each) == int:\n",
    "                embed_ls[i] = torch.zeros((300,), dtype=torch.float32)\n",
    "            elif type(each) == np.ndarray:\n",
    "                embed_ls[i] = torch.tensor(each, dtype=torch.float32)\n",
    "        \n",
    "        x_torch = torch.stack(embed_ls, dim=0)\n",
    "        # if index == 0:\n",
    "        #     print(embed_ls)\n",
    "        return x_torch, self.data[index]['labels'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 300])\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "dataset = lstm_model_dataset(data_train)\n",
    "dataloader = DataLoader(lstm_model_dataset)\n",
    "for i, (x, y) in enumerate(dataset):\n",
    "    if i == 0:\n",
    "        print(x.shape)\n",
    "        print(y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, inode, hnode) -> None:\n",
    "        super().__init__()\n",
    "        self.cell = nn.LSTMCell(inode, hnode)\n",
    "        self.inode = inode\n",
    "        self.hnode = hnode\n",
    "\n",
    "    def forward(self, x, hc_0=None):\n",
    "        dtype = x.dtype\n",
    "        device = x.device\n",
    "        batch_size, seq_len, inode = x.shape\n",
    "        if hc_0 is not None:\n",
    "            h_0, c_0 = hc_0\n",
    "        else:\n",
    "            h_0 = torch.randn((batch_size, self.hnode))\n",
    "            c_0 = torch.randn((batch_size, self.hnode))\n",
    "\n",
    "        h_t_1 = h_0\n",
    "        c_t_1 = c_0\n",
    "        h_list = []\n",
    "        for t in range(seq_len):\n",
    "            #x_t.shape = (batch_size, inode)\n",
    "            x_t = x[:, t, :]\n",
    "            h_t, c_t = self.cell(x_t, (h_t_1, c_t_1))\n",
    "\n",
    "            h_list.append(h_t)\n",
    "            h_t_1, c_t_1 = h_t, c_t\n",
    "\n",
    "        \"\"\"调整张量格式\"\"\"\n",
    "        # h_list: [(batch_size, hnode)]*seq_len\n",
    "        h = torch.stack(h_list, dim=1) # h.shape = (batch_size, seq_len, hnode)\n",
    "        return h, (h_t_1, c_t_1)\n",
    "        \n",
    "class Embedding_lstm(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_vocab, # num_embeddings\n",
    "                 pad_size, # seq_len \n",
    "                 inode, # embedding_dim\n",
    "                 hnode,\n",
    "                 num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, inode) # Embedding 输入 one-hot 格式的向量\n",
    "        #(batch_size, seq_len, inode)\n",
    "        self.lstm = LSTM1(inode, hnode)\n",
    "        #(batch_size, hnode)\n",
    "        self.fc = nn.Linear(hnode, num_classes)\n",
    "        # (batch_size, num_classes)\n",
    "        self.pad_size = pad_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding(x)\n",
    "\n",
    "        #h.shape = (batch_size, pad_size, hnode)\n",
    "        h, _ = self.lstm(embedding)\n",
    "        h_last = h[:, -1, :]\n",
    "        # h_last.shape = (batch_size, hnode)\n",
    "\n",
    "        # result.shape = (batch_size, num_classes)\n",
    "        result = self.fc(h_last)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 300\n",
    "HIDDEN_SIZE = 600\n",
    "NUM_CLASSES = len(classes)\n",
    "NUM_LAYERS = 2\n",
    "SEQ_LEN = 32 # PAD_SIZE\n",
    "\n",
    "LR = 0.001\n",
    "NUM_EPOCH = 100\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Embedding层预处理数据，输入为单词的indice形式\n",
    "\n",
    "# 假设我们有以下句子\n",
    "sentence = \"I love natural language processing\"\n",
    "\n",
    "# 创建词汇表\n",
    "vocab = set(sentence.split())\n",
    "\n",
    "# 分配单词编码\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# 将句子转换为one-hot编码\n",
    "one_hot = []\n",
    "for word in sentence.split():\n",
    "    vec = [0] * len(vocab)\n",
    "    vec[word_to_ix[word]] = 1\n",
    "    one_hot.append(vec)\n",
    "\n",
    "# 输出结果\n",
    "print(one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab_Link:\n",
    "    def __init__(self, with_pad=True) -> None:\n",
    "        self.vocab = set()\n",
    "        self.word_to_ix = {}\n",
    "        self.ix_to_word = {}\n",
    "        self.num = 0\n",
    "        self.with_pad = with_pad\n",
    "        if with_pad:\n",
    "            self.word_to_ix['<PAD>'] = 0\n",
    "            self.ix_to_word['0'] = '<PAD>'\n",
    "            self.num = 1\n",
    "\n",
    "    def add_sentence(self, s: str):\n",
    "        sentence_vocab =set(s.split())\n",
    "        add_list = []\n",
    "        for i in sentence_vocab:\n",
    "            if i not in self.vocab:\n",
    "                add_list.append(i)\n",
    "        self.vocab = self.vocab.union(add_list)\n",
    "        # self.word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "        # self.ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "        for ix, word in enumerate(add_list):\n",
    "            if self.with_pad:\n",
    "                ix = ix + 1\n",
    "            self.word_to_ix[word] = ix + self.num\n",
    "            self.ix_to_word[ix + self.num] = word\n",
    "        \n",
    "        self.num += len(add_list)\n",
    "\n",
    "    def to_one_hot(self, s: str):\n",
    "        one_hot = []\n",
    "        for word in s.split():\n",
    "            vec = [0] * self.num\n",
    "            vec[word_to_ix[word]] = 1\n",
    "            one_hot.append(vec)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I’m really sorry about your situation :( Although I love the names Sapphira, Cirilla, and Scarlett!', 'labels': [25], 'id': 'eecwqtt'}\n"
     ]
    }
   ],
   "source": [
    "print(data_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done train data: 56730\n",
      "done val data: 61267\n",
      "done test data: 65524\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab_Link()\n",
    "for each in data_train:\n",
    "    sentence = each['text']\n",
    "    vocab.add_sentence(sentence)\n",
    "print('done train data:', len(vocab))\n",
    "\n",
    "for each in data_val:\n",
    "    sentence = each['text']\n",
    "    vocab.add_sentence(sentence)\n",
    "print('done val data:', len(vocab))\n",
    "\n",
    "\n",
    "for each in data_test:\n",
    "    sentence = each['text']\n",
    "    vocab.add_sentence(sentence)\n",
    "print('done test data:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords_english.txt','r') as f:\n",
    "    stop_words = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = LSTM_Model(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_Model(\n",
       "  (lstm): LSTM(300, 600, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=600, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1357"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_0 = lstm_model_dataset(data_train)\n",
    "train_dataloader_0 = DataLoader(dataset_0, batch_size=BATCH_SIZE)\n",
    "len(train_dataloader_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Embedding_lstm(n_vocab=len(vocab), pad_size=SEQ_LEN, inode=INPUT_SIZE, hnode=HIDDEN_SIZE, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Embedding_lstm_input(Dataset):\n",
    "    def __init__(self, data) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        words = self.data['text'][index].split()\n",
    "        if len(words) > SEQ_LEN:\n",
    "            words = words[:SEQ_LEN]\n",
    "        elif len(words) < SEQ_LEN:\n",
    "            words.extend(['<PAD>'] * (SEQ_LEN-len(words)))\n",
    "        indices = [vocab.word_to_ix[each] for each in words]\n",
    "        label = torch.tensor(self.data['labels'][index][0], dtype=torch.long)\n",
    "        return torch.tensor(indices, dtype=torch.long), label  # (pad_size,) , label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Embedding_lstm_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m input_train \u001b[39m=\u001b[39m DataLoader(Embedding_lstm_input(data_train[\u001b[39m0\u001b[39m:\u001b[39m1000\u001b[39m]), batch_size\u001b[39m=\u001b[39mBATCH_SIZE)\n\u001b[0;32m      2\u001b[0m input_val \u001b[39m=\u001b[39m DataLoader(Embedding_lstm_input(data_val[\u001b[39m0\u001b[39m:\u001b[39m1000\u001b[39m]))\n\u001b[0;32m      3\u001b[0m \u001b[39mlen\u001b[39m(input_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Embedding_lstm_input' is not defined"
     ]
    }
   ],
   "source": [
    "input_train = DataLoader(Embedding_lstm_input(data_train[0:1000]), batch_size=BATCH_SIZE)\n",
    "input_val = DataLoader(Embedding_lstm_input(data_val[0:1000]))\n",
    "len(input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  960,   294,   140,  3801, 56731,    11,   418,    39,   188,   294,\n",
       "             2,  3801, 31481,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor(27)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_val_list = []\n",
    "for x, y in input_val:\n",
    "    input_val_list.append([x.squeeze(0), y.squeeze(0)])\n",
    "input_val_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6306)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion(torch.randn((10,)),torch.tensor([1,0,0,0,0,0,0,0,0,0], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_train(model, # nn.Modules derived manual ML model\n",
    "          loader,\n",
    "          criterion=None, optimizer=None, batch_size=10, epoches=100, lr=0.01, print_format='normal',\n",
    "          loss_appendix=False, # if add para_cal to loss\n",
    "          graph=True, \n",
    "          y_label=True, # if y is label or one-hot coding\n",
    "          model_desp=False, # if has .inode / .onode / .hnode_stringformat attrs\n",
    "          val=False,\n",
    "          val_loader=None,\n",
    "          batch_skip=10, # how many batches show a loss\n",
    "          device = 'cpu'\n",
    "          ):\n",
    "    total_loss = 0 # loss per epoch\n",
    "    avg_loss = [] # loss per batch\n",
    "    total_loss_val = 0 # loss per epoch\n",
    "    loss_val = [] # loss per idx\n",
    "    \n",
    "    if criterion == None:\n",
    "        criterion = nn.MSELoss()\n",
    "    if optimizer == None:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    # 避免显示太长\n",
    "    if epoches <= 100:\n",
    "        epoch_interval = 1\n",
    "    elif epoches <= 300:\n",
    "        epoch_interval = 5\n",
    "    elif epoches <= 500:\n",
    "        epoch_interval = 10\n",
    "    else:\n",
    "        epoch_interval = 20\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    logging('\\n')\n",
    "    logging(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "    for epoch in range(epoches):\n",
    "        total_loss = 0\n",
    "        for batch, (x_torch, y_torch) in enumerate(loader):\n",
    "            start = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            # x_torch = torch.from_numpy(x).to(torch.float32).requires_grad_()\n",
    "            x_torch = x_torch.to(device)\n",
    "            # if batch == 0:\n",
    "            #     print(x_torch)\n",
    "            pred = model(x_torch)\n",
    "            # pred = torch.argmax(model(x_torch), dim=1)\n",
    "            # wrong\n",
    "            if y_label:\n",
    "                y_torch_n = torch.zeros(pred.shape, dtype=torch.float)\n",
    "                for i in range(y_torch_n.shape[0]):\n",
    "                    y_torch_n[i][y_torch[i]] = 1\n",
    "                y_torch = y_torch_n\n",
    "            y_torch = y_torch.to(device)\n",
    "            loss = criterion(pred, y_torch)\n",
    "            if loss_appendix: # regularzation\n",
    "                pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()*x_torch.shape[0]\n",
    "            avg_loss.append(loss)\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            if batch % 100 == 0 and epoch % epoch_interval == 0:   \n",
    "                if print_format == 'time':\n",
    "                    logging('| epoch {:3d} | {:5d}/{:5d} batches | lr {:05.5f} | ms/batch {:5.2f} | '\n",
    "                            'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                        epoch, batch, len(loader), optimizer.param_groups[0]['lr'],\n",
    "                        elapsed * 1000 / x_torch.size(0), loss, torch.exp(loss)))\n",
    "                else:\n",
    "                    logging('| epoch {:3d} | {:5d}/{:5d} batches | lr {:05.5f} | '\n",
    "                            'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                        epoch, batch, len(loader), optimizer.param_groups[0]['lr'],\n",
    "                        loss, torch.exp(loss)))\n",
    "                \n",
    "        if val:\n",
    "            total_loss_val = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                start_val = time.time()\n",
    "                for x_torch, y_torch in val_loader:\n",
    "                    x_torch = x_torch.to(device)\n",
    "                    pred = model(x_torch)\n",
    "                    # pred = torch.argmax(model(x_torch), dim=1)\n",
    "                    if y_label:\n",
    "                        y_torch_n = torch.zeros(pred.shape)\n",
    "                        for i in range(y_torch_n.shape[0]):\n",
    "                            y_torch_n[i][y_torch[i]] = 1\n",
    "                        y_torch = y_torch_n\n",
    "                    y_torch = y_torch.to(device)\n",
    "                    loss = criterion(pred, y_torch)\n",
    "                    if loss_appendix:\n",
    "                        pass\n",
    "                    # loss.backward()\n",
    "                    # optimizer.step()\n",
    "\n",
    "                    total_loss_val += loss\n",
    "                    loss_val.append(loss)\n",
    "                    \n",
    "                loss = total_loss_val / len(val_loader)\n",
    "                # val 不按照batch计算\n",
    "                elapsed = time.time() - start_val\n",
    "                if print_format == 'time':\n",
    "                    logging('val: | epoch {:3d} | lr {:05.5f} | ms/epoch {:5.2f} | '\n",
    "                            'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                        epoch, optimizer.param_groups[0]['lr'],\n",
    "                        elapsed * 1000, loss, torch.exp(loss)))\n",
    "                else:\n",
    "                    logging('val: | epoch {:3d} | lr {:05.5f} | '\n",
    "                            'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                        epoch, optimizer.param_groups[0]['lr'],\n",
    "                        loss, torch.exp(loss)))\n",
    "                \n",
    "    if model_desp:\n",
    "        name = 'I:{:3d}, H:{}, O:{:3d}, lr:{:02.2f}, epoch:{:5d}'.format(\n",
    "            model.inode, model.hnode_stringformat, model.onode, optimizer.param_groups[0]['lr'], epoches)\n",
    "    else:\n",
    "        name = 'model_0'\n",
    "                \n",
    "    if graph:\n",
    "        my_plt(avg_loss, 'loss', name)\n",
    "        my_plt(avg_loss, 'loss_mean', name, mean=True)\n",
    "        if val:\n",
    "            my_plt(loss_val, 'val_loss', name)\n",
    "            my_plt(loss_val, 'val_loss_mean', name, mean=True)\n",
    "        \n",
    "    # logging('\\n' + str(summary(model, loader.shape)))\n",
    "    logging('\\n' + str(model))\n",
    "    \n",
    "    if val:\n",
    "        return (total_loss, avg_loss) , (total_loss_val, loss_val)\n",
    "    return total_loss, avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "\n",
      "\n",
      "2023-06-26 14:28:54\n",
      "| epoch   0 |     0/ 1357 batches | lr 0.00 | loss  2.54 | ppl    12.62\n",
      "| epoch   0 |   100/ 1357 batches | lr 0.00 | loss  2.70 | ppl    14.94\n",
      "| epoch   0 |   200/ 1357 batches | lr 0.00 | loss  2.22 | ppl     9.18\n",
      "| epoch   0 |   300/ 1357 batches | lr 0.00 | loss  2.09 | ppl     8.10\n",
      "| epoch   0 |   400/ 1357 batches | lr 0.00 | loss  2.55 | ppl    12.78\n",
      "| epoch   0 |   500/ 1357 batches | lr 0.00 | loss  2.26 | ppl     9.63\n",
      "| epoch   0 |   600/ 1357 batches | lr 0.00 | loss  2.06 | ppl     7.86\n",
      "| epoch   0 |   700/ 1357 batches | lr 0.00 | loss  2.42 | ppl    11.29\n",
      "| epoch   0 |   800/ 1357 batches | lr 0.00 | loss  2.32 | ppl    10.14\n",
      "| epoch   0 |   900/ 1357 batches | lr 0.00 | loss  2.60 | ppl    13.46\n",
      "| epoch   0 |  1000/ 1357 batches | lr 0.00 | loss  2.07 | ppl     7.96\n",
      "| epoch   0 |  1100/ 1357 batches | lr 0.00 | loss  2.55 | ppl    12.85\n",
      "| epoch   0 |  1200/ 1357 batches | lr 0.00 | loss  2.25 | ppl     9.47\n",
      "| epoch   0 |  1300/ 1357 batches | lr 0.00 | loss  2.05 | ppl     7.74\n",
      "| epoch   1 |     0/ 1357 batches | lr 0.00 | loss  2.17 | ppl     8.74\n",
      "| epoch   1 |   100/ 1357 batches | lr 0.00 | loss  2.45 | ppl    11.59\n",
      "| epoch   1 |   200/ 1357 batches | lr 0.00 | loss  1.98 | ppl     7.23\n",
      "| epoch   1 |   300/ 1357 batches | lr 0.00 | loss  1.90 | ppl     6.69\n",
      "| epoch   1 |   400/ 1357 batches | lr 0.00 | loss  2.06 | ppl     7.81\n",
      "| epoch   1 |   500/ 1357 batches | lr 0.00 | loss  1.97 | ppl     7.14\n",
      "| epoch   1 |   600/ 1357 batches | lr 0.00 | loss  1.62 | ppl     5.06\n",
      "| epoch   1 |   700/ 1357 batches | lr 0.00 | loss  2.14 | ppl     8.47\n",
      "| epoch   1 |   800/ 1357 batches | lr 0.00 | loss  1.81 | ppl     6.09\n",
      "| epoch   1 |   900/ 1357 batches | lr 0.00 | loss  2.50 | ppl    12.22\n",
      "| epoch   1 |  1000/ 1357 batches | lr 0.00 | loss  2.07 | ppl     7.95\n",
      "| epoch   1 |  1100/ 1357 batches | lr 0.00 | loss  2.25 | ppl     9.45\n",
      "| epoch   1 |  1200/ 1357 batches | lr 0.00 | loss  2.23 | ppl     9.30\n",
      "| epoch   1 |  1300/ 1357 batches | lr 0.00 | loss  2.00 | ppl     7.38\n",
      "| epoch   2 |     0/ 1357 batches | lr 0.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   2 |   100/ 1357 batches | lr 0.00 | loss  2.23 | ppl     9.27\n",
      "| epoch   2 |   200/ 1357 batches | lr 0.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   2 |   300/ 1357 batches | lr 0.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   2 |   400/ 1357 batches | lr 0.00 | loss  1.99 | ppl     7.30\n",
      "| epoch   2 |   500/ 1357 batches | lr 0.00 | loss  1.99 | ppl     7.34\n",
      "| epoch   2 |   600/ 1357 batches | lr 0.00 | loss  1.48 | ppl     4.38\n",
      "| epoch   2 |   700/ 1357 batches | lr 0.00 | loss  1.96 | ppl     7.10\n",
      "| epoch   2 |   800/ 1357 batches | lr 0.00 | loss  1.68 | ppl     5.35\n",
      "| epoch   2 |   900/ 1357 batches | lr 0.00 | loss  2.40 | ppl    11.07\n",
      "| epoch   2 |  1000/ 1357 batches | lr 0.00 | loss  1.81 | ppl     6.10\n",
      "| epoch   2 |  1100/ 1357 batches | lr 0.00 | loss  2.03 | ppl     7.62\n",
      "| epoch   2 |  1200/ 1357 batches | lr 0.00 | loss  2.10 | ppl     8.14\n",
      "| epoch   2 |  1300/ 1357 batches | lr 0.00 | loss  1.93 | ppl     6.92\n",
      "| epoch   3 |     0/ 1357 batches | lr 0.00 | loss  1.54 | ppl     4.68\n",
      "| epoch   3 |   100/ 1357 batches | lr 0.00 | loss  2.15 | ppl     8.54\n",
      "| epoch   3 |   200/ 1357 batches | lr 0.00 | loss  1.64 | ppl     5.14\n",
      "| epoch   3 |   300/ 1357 batches | lr 0.00 | loss  1.69 | ppl     5.44\n",
      "| epoch   3 |   400/ 1357 batches | lr 0.00 | loss  1.82 | ppl     6.17\n",
      "| epoch   3 |   500/ 1357 batches | lr 0.00 | loss  1.86 | ppl     6.45\n",
      "| epoch   3 |   600/ 1357 batches | lr 0.00 | loss  1.41 | ppl     4.11\n",
      "| epoch   3 |   700/ 1357 batches | lr 0.00 | loss  1.75 | ppl     5.73\n",
      "| epoch   3 |   800/ 1357 batches | lr 0.00 | loss  1.48 | ppl     4.41\n",
      "| epoch   3 |   900/ 1357 batches | lr 0.00 | loss  2.33 | ppl    10.25\n",
      "| epoch   3 |  1000/ 1357 batches | lr 0.00 | loss  1.67 | ppl     5.30\n",
      "| epoch   3 |  1100/ 1357 batches | lr 0.00 | loss  1.83 | ppl     6.22\n",
      "| epoch   3 |  1200/ 1357 batches | lr 0.00 | loss  2.04 | ppl     7.71\n",
      "| epoch   3 |  1300/ 1357 batches | lr 0.00 | loss  1.87 | ppl     6.51\n",
      "| epoch   4 |     0/ 1357 batches | lr 0.00 | loss  1.30 | ppl     3.66\n",
      "| epoch   4 |   100/ 1357 batches | lr 0.00 | loss  2.06 | ppl     7.88\n",
      "| epoch   4 |   200/ 1357 batches | lr 0.00 | loss  1.61 | ppl     5.00\n",
      "| epoch   4 |   300/ 1357 batches | lr 0.00 | loss  1.64 | ppl     5.15\n",
      "| epoch   4 |   400/ 1357 batches | lr 0.00 | loss  1.75 | ppl     5.78\n",
      "| epoch   4 |   500/ 1357 batches | lr 0.00 | loss  1.83 | ppl     6.22\n",
      "| epoch   4 |   600/ 1357 batches | lr 0.00 | loss  1.31 | ppl     3.72\n",
      "| epoch   4 |   700/ 1357 batches | lr 0.00 | loss  1.58 | ppl     4.86\n",
      "| epoch   4 |   800/ 1357 batches | lr 0.00 | loss  1.36 | ppl     3.90\n",
      "| epoch   4 |   900/ 1357 batches | lr 0.00 | loss  2.15 | ppl     8.55\n",
      "| epoch   4 |  1000/ 1357 batches | lr 0.00 | loss  1.59 | ppl     4.91\n",
      "| epoch   4 |  1100/ 1357 batches | lr 0.00 | loss  1.66 | ppl     5.26\n",
      "| epoch   4 |  1200/ 1357 batches | lr 0.00 | loss  1.94 | ppl     6.95\n",
      "| epoch   4 |  1300/ 1357 batches | lr 0.00 | loss  1.80 | ppl     6.04\n",
      "| epoch   5 |     0/ 1357 batches | lr 0.00 | loss  1.03 | ppl     2.79\n",
      "| epoch   5 |   100/ 1357 batches | lr 0.00 | loss  1.90 | ppl     6.67\n",
      "| epoch   5 |   200/ 1357 batches | lr 0.00 | loss  1.48 | ppl     4.39\n",
      "| epoch   5 |   300/ 1357 batches | lr 0.00 | loss  1.63 | ppl     5.11\n",
      "| epoch   5 |   400/ 1357 batches | lr 0.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   5 |   500/ 1357 batches | lr 0.00 | loss  1.84 | ppl     6.32\n",
      "| epoch   5 |   600/ 1357 batches | lr 0.00 | loss  1.11 | ppl     3.03\n",
      "| epoch   5 |   700/ 1357 batches | lr 0.00 | loss  1.43 | ppl     4.19\n",
      "| epoch   5 |   800/ 1357 batches | lr 0.00 | loss  1.20 | ppl     3.31\n",
      "| epoch   5 |   900/ 1357 batches | lr 0.00 | loss  2.06 | ppl     7.82\n",
      "| epoch   5 |  1000/ 1357 batches | lr 0.00 | loss  1.48 | ppl     4.40\n",
      "| epoch   5 |  1100/ 1357 batches | lr 0.00 | loss  1.51 | ppl     4.51\n",
      "| epoch   5 |  1200/ 1357 batches | lr 0.00 | loss  1.65 | ppl     5.21\n",
      "| epoch   5 |  1300/ 1357 batches | lr 0.00 | loss  1.61 | ppl     5.01\n",
      "| epoch   6 |     0/ 1357 batches | lr 0.00 | loss  0.90 | ppl     2.45\n",
      "| epoch   6 |   100/ 1357 batches | lr 0.00 | loss  1.63 | ppl     5.11\n",
      "| epoch   6 |   200/ 1357 batches | lr 0.00 | loss  1.34 | ppl     3.81\n",
      "| epoch   6 |   300/ 1357 batches | lr 0.00 | loss  1.63 | ppl     5.12\n",
      "| epoch   6 |   400/ 1357 batches | lr 0.00 | loss  1.74 | ppl     5.72\n",
      "| epoch   6 |   500/ 1357 batches | lr 0.00 | loss  1.77 | ppl     5.86\n",
      "| epoch   6 |   600/ 1357 batches | lr 0.00 | loss  0.92 | ppl     2.52\n",
      "| epoch   6 |   700/ 1357 batches | lr 0.00 | loss  1.27 | ppl     3.57\n",
      "| epoch   6 |   800/ 1357 batches | lr 0.00 | loss  0.97 | ppl     2.64\n",
      "| epoch   6 |   900/ 1357 batches | lr 0.00 | loss  2.04 | ppl     7.71\n",
      "| epoch   6 |  1000/ 1357 batches | lr 0.00 | loss  1.36 | ppl     3.89\n",
      "| epoch   6 |  1100/ 1357 batches | lr 0.00 | loss  1.29 | ppl     3.64\n",
      "| epoch   6 |  1200/ 1357 batches | lr 0.00 | loss  1.48 | ppl     4.38\n",
      "| epoch   6 |  1300/ 1357 batches | lr 0.00 | loss  1.46 | ppl     4.32\n",
      "| epoch   7 |     0/ 1357 batches | lr 0.00 | loss  0.86 | ppl     2.36\n",
      "| epoch   7 |   100/ 1357 batches | lr 0.00 | loss  1.35 | ppl     3.85\n",
      "| epoch   7 |   200/ 1357 batches | lr 0.00 | loss  1.20 | ppl     3.32\n",
      "| epoch   7 |   300/ 1357 batches | lr 0.00 | loss  1.44 | ppl     4.20\n",
      "| epoch   7 |   400/ 1357 batches | lr 0.00 | loss  1.68 | ppl     5.35\n",
      "| epoch   7 |   500/ 1357 batches | lr 0.00 | loss  1.69 | ppl     5.40\n",
      "| epoch   7 |   600/ 1357 batches | lr 0.00 | loss  0.91 | ppl     2.49\n",
      "| epoch   7 |   700/ 1357 batches | lr 0.00 | loss  1.08 | ppl     2.95\n",
      "| epoch   7 |   800/ 1357 batches | lr 0.00 | loss  0.92 | ppl     2.50\n",
      "| epoch   7 |   900/ 1357 batches | lr 0.00 | loss  1.70 | ppl     5.49\n",
      "| epoch   7 |  1000/ 1357 batches | lr 0.00 | loss  1.11 | ppl     3.05\n",
      "| epoch   7 |  1100/ 1357 batches | lr 0.00 | loss  1.04 | ppl     2.82\n",
      "| epoch   7 |  1200/ 1357 batches | lr 0.00 | loss  1.35 | ppl     3.87\n",
      "| epoch   7 |  1300/ 1357 batches | lr 0.00 | loss  1.29 | ppl     3.64\n",
      "| epoch   8 |     0/ 1357 batches | lr 0.00 | loss  0.63 | ppl     1.87\n",
      "| epoch   8 |   100/ 1357 batches | lr 0.00 | loss  1.08 | ppl     2.93\n",
      "| epoch   8 |   200/ 1357 batches | lr 0.00 | loss  0.91 | ppl     2.48\n",
      "| epoch   8 |   300/ 1357 batches | lr 0.00 | loss  1.30 | ppl     3.66\n",
      "| epoch   8 |   400/ 1357 batches | lr 0.00 | loss  1.57 | ppl     4.81\n",
      "| epoch   8 |   500/ 1357 batches | lr 0.00 | loss  1.58 | ppl     4.87\n",
      "| epoch   8 |   600/ 1357 batches | lr 0.00 | loss  0.80 | ppl     2.22\n",
      "| epoch   8 |   700/ 1357 batches | lr 0.00 | loss  1.03 | ppl     2.79\n",
      "| epoch   8 |   800/ 1357 batches | lr 0.00 | loss  0.70 | ppl     2.01\n",
      "| epoch   8 |   900/ 1357 batches | lr 0.00 | loss  1.68 | ppl     5.36\n",
      "| epoch   8 |  1000/ 1357 batches | lr 0.00 | loss  0.91 | ppl     2.48\n",
      "| epoch   8 |  1100/ 1357 batches | lr 0.00 | loss  0.88 | ppl     2.42\n",
      "| epoch   8 |  1200/ 1357 batches | lr 0.00 | loss  1.13 | ppl     3.10\n",
      "| epoch   8 |  1300/ 1357 batches | lr 0.00 | loss  0.98 | ppl     2.66\n",
      "| epoch   9 |     0/ 1357 batches | lr 0.00 | loss  0.43 | ppl     1.54\n",
      "| epoch   9 |   100/ 1357 batches | lr 0.00 | loss  0.77 | ppl     2.16\n",
      "| epoch   9 |   200/ 1357 batches | lr 0.00 | loss  0.90 | ppl     2.47\n",
      "| epoch   9 |   300/ 1357 batches | lr 0.00 | loss  1.17 | ppl     3.23\n",
      "| epoch   9 |   400/ 1357 batches | lr 0.00 | loss  1.14 | ppl     3.11\n",
      "| epoch   9 |   500/ 1357 batches | lr 0.00 | loss  1.34 | ppl     3.82\n",
      "| epoch   9 |   600/ 1357 batches | lr 0.00 | loss  0.77 | ppl     2.17\n",
      "| epoch   9 |   700/ 1357 batches | lr 0.00 | loss  0.75 | ppl     2.11\n",
      "| epoch   9 |   800/ 1357 batches | lr 0.00 | loss  0.72 | ppl     2.06\n",
      "| epoch   9 |   900/ 1357 batches | lr 0.00 | loss  1.49 | ppl     4.46\n",
      "| epoch   9 |  1000/ 1357 batches | lr 0.00 | loss  0.74 | ppl     2.09\n",
      "| epoch   9 |  1100/ 1357 batches | lr 0.00 | loss  0.84 | ppl     2.31\n",
      "| epoch   9 |  1200/ 1357 batches | lr 0.00 | loss  1.04 | ppl     2.84\n",
      "| epoch   9 |  1300/ 1357 batches | lr 0.00 | loss  0.82 | ppl     2.27\n",
      "| epoch  10 |     0/ 1357 batches | lr 0.00 | loss  0.39 | ppl     1.47\n",
      "| epoch  10 |   100/ 1357 batches | lr 0.00 | loss  0.80 | ppl     2.22\n",
      "| epoch  10 |   200/ 1357 batches | lr 0.00 | loss  0.78 | ppl     2.19\n",
      "| epoch  10 |   300/ 1357 batches | lr 0.00 | loss  0.97 | ppl     2.63\n",
      "| epoch  10 |   400/ 1357 batches | lr 0.00 | loss  1.40 | ppl     4.05\n",
      "| epoch  10 |   500/ 1357 batches | lr 0.00 | loss  1.11 | ppl     3.05\n",
      "| epoch  10 |   600/ 1357 batches | lr 0.00 | loss  0.64 | ppl     1.89\n",
      "| epoch  10 |   700/ 1357 batches | lr 0.00 | loss  0.69 | ppl     2.00\n",
      "| epoch  10 |   800/ 1357 batches | lr 0.00 | loss  0.51 | ppl     1.67\n",
      "| epoch  10 |   900/ 1357 batches | lr 0.00 | loss  1.14 | ppl     3.13\n",
      "| epoch  10 |  1000/ 1357 batches | lr 0.00 | loss  0.55 | ppl     1.73\n",
      "| epoch  10 |  1100/ 1357 batches | lr 0.00 | loss  0.52 | ppl     1.69\n",
      "| epoch  10 |  1200/ 1357 batches | lr 0.00 | loss  1.15 | ppl     3.14\n",
      "| epoch  10 |  1300/ 1357 batches | lr 0.00 | loss  0.57 | ppl     1.78\n",
      "| epoch  11 |     0/ 1357 batches | lr 0.00 | loss  0.34 | ppl     1.40\n",
      "| epoch  11 |   100/ 1357 batches | lr 0.00 | loss  0.64 | ppl     1.90\n",
      "| epoch  11 |   200/ 1357 batches | lr 0.00 | loss  0.57 | ppl     1.76\n",
      "| epoch  11 |   300/ 1357 batches | lr 0.00 | loss  0.99 | ppl     2.69\n",
      "| epoch  11 |   400/ 1357 batches | lr 0.00 | loss  1.06 | ppl     2.89\n",
      "| epoch  11 |   500/ 1357 batches | lr 0.00 | loss  0.81 | ppl     2.25\n",
      "| epoch  11 |   600/ 1357 batches | lr 0.00 | loss  0.42 | ppl     1.52\n",
      "| epoch  11 |   700/ 1357 batches | lr 0.00 | loss  0.46 | ppl     1.58\n",
      "| epoch  11 |   800/ 1357 batches | lr 0.00 | loss  0.43 | ppl     1.53\n",
      "| epoch  11 |   900/ 1357 batches | lr 0.00 | loss  0.83 | ppl     2.28\n",
      "| epoch  11 |  1000/ 1357 batches | lr 0.00 | loss  0.43 | ppl     1.54\n",
      "| epoch  11 |  1100/ 1357 batches | lr 0.00 | loss  0.44 | ppl     1.56\n",
      "| epoch  11 |  1200/ 1357 batches | lr 0.00 | loss  1.04 | ppl     2.83\n",
      "| epoch  11 |  1300/ 1357 batches | lr 0.00 | loss  0.43 | ppl     1.53\n",
      "| epoch  12 |     0/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.26\n",
      "| epoch  12 |   100/ 1357 batches | lr 0.00 | loss  0.33 | ppl     1.39\n",
      "| epoch  12 |   200/ 1357 batches | lr 0.00 | loss  0.58 | ppl     1.79\n",
      "| epoch  12 |   300/ 1357 batches | lr 0.00 | loss  0.62 | ppl     1.86\n",
      "| epoch  12 |   400/ 1357 batches | lr 0.00 | loss  0.53 | ppl     1.70\n",
      "| epoch  12 |   500/ 1357 batches | lr 0.00 | loss  0.69 | ppl     1.99\n",
      "| epoch  12 |   600/ 1357 batches | lr 0.00 | loss  0.42 | ppl     1.52\n",
      "| epoch  12 |   700/ 1357 batches | lr 0.00 | loss  0.59 | ppl     1.81\n",
      "| epoch  12 |   800/ 1357 batches | lr 0.00 | loss  0.31 | ppl     1.37\n",
      "| epoch  12 |   900/ 1357 batches | lr 0.00 | loss  1.22 | ppl     3.40\n",
      "| epoch  12 |  1000/ 1357 batches | lr 0.00 | loss  0.44 | ppl     1.56\n",
      "| epoch  12 |  1100/ 1357 batches | lr 0.00 | loss  0.53 | ppl     1.70\n",
      "| epoch  12 |  1200/ 1357 batches | lr 0.00 | loss  0.76 | ppl     2.14\n",
      "| epoch  12 |  1300/ 1357 batches | lr 0.00 | loss  0.40 | ppl     1.49\n",
      "| epoch  13 |     0/ 1357 batches | lr 0.00 | loss  0.34 | ppl     1.41\n",
      "| epoch  13 |   100/ 1357 batches | lr 0.00 | loss  0.37 | ppl     1.44\n",
      "| epoch  13 |   200/ 1357 batches | lr 0.00 | loss  0.54 | ppl     1.72\n",
      "| epoch  13 |   300/ 1357 batches | lr 0.00 | loss  0.43 | ppl     1.54\n",
      "| epoch  13 |   400/ 1357 batches | lr 0.00 | loss  0.46 | ppl     1.58\n",
      "| epoch  13 |   500/ 1357 batches | lr 0.00 | loss  0.52 | ppl     1.68\n",
      "| epoch  13 |   600/ 1357 batches | lr 0.00 | loss  0.40 | ppl     1.48\n",
      "| epoch  13 |   700/ 1357 batches | lr 0.00 | loss  0.43 | ppl     1.54\n",
      "| epoch  13 |   800/ 1357 batches | lr 0.00 | loss  0.31 | ppl     1.36\n",
      "| epoch  13 |   900/ 1357 batches | lr 0.00 | loss  0.48 | ppl     1.61\n",
      "| epoch  13 |  1000/ 1357 batches | lr 0.00 | loss  0.52 | ppl     1.68\n",
      "| epoch  13 |  1100/ 1357 batches | lr 0.00 | loss  0.37 | ppl     1.44\n",
      "| epoch  13 |  1200/ 1357 batches | lr 0.00 | loss  0.87 | ppl     2.39\n",
      "| epoch  13 |  1300/ 1357 batches | lr 0.00 | loss  0.29 | ppl     1.33\n",
      "| epoch  14 |     0/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.16\n",
      "| epoch  14 |   100/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.25\n",
      "| epoch  14 |   200/ 1357 batches | lr 0.00 | loss  0.41 | ppl     1.50\n",
      "| epoch  14 |   300/ 1357 batches | lr 0.00 | loss  0.33 | ppl     1.39\n",
      "| epoch  14 |   400/ 1357 batches | lr 0.00 | loss  0.42 | ppl     1.53\n",
      "| epoch  14 |   500/ 1357 batches | lr 0.00 | loss  0.33 | ppl     1.40\n",
      "| epoch  14 |   600/ 1357 batches | lr 0.00 | loss  0.30 | ppl     1.35\n",
      "| epoch  14 |   700/ 1357 batches | lr 0.00 | loss  0.27 | ppl     1.31\n",
      "| epoch  14 |   800/ 1357 batches | lr 0.00 | loss  0.36 | ppl     1.44\n",
      "| epoch  14 |   900/ 1357 batches | lr 0.00 | loss  0.37 | ppl     1.45\n",
      "| epoch  14 |  1000/ 1357 batches | lr 0.00 | loss  0.45 | ppl     1.57\n",
      "| epoch  14 |  1100/ 1357 batches | lr 0.00 | loss  0.42 | ppl     1.53\n",
      "| epoch  14 |  1200/ 1357 batches | lr 0.00 | loss  0.68 | ppl     1.97\n",
      "| epoch  14 |  1300/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.28\n",
      "| epoch  15 |     0/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.25\n",
      "| epoch  15 |   100/ 1357 batches | lr 0.00 | loss  0.32 | ppl     1.38\n",
      "| epoch  15 |   200/ 1357 batches | lr 0.00 | loss  0.62 | ppl     1.85\n",
      "| epoch  15 |   300/ 1357 batches | lr 0.00 | loss  0.43 | ppl     1.54\n",
      "| epoch  15 |   400/ 1357 batches | lr 0.00 | loss  0.28 | ppl     1.33\n",
      "| epoch  15 |   500/ 1357 batches | lr 0.00 | loss  0.45 | ppl     1.56\n",
      "| epoch  15 |   600/ 1357 batches | lr 0.00 | loss  0.36 | ppl     1.44\n",
      "| epoch  15 |   700/ 1357 batches | lr 0.00 | loss  0.64 | ppl     1.90\n",
      "| epoch  15 |   800/ 1357 batches | lr 0.00 | loss  0.26 | ppl     1.30\n",
      "| epoch  15 |   900/ 1357 batches | lr 0.00 | loss  0.40 | ppl     1.50\n",
      "| epoch  15 |  1000/ 1357 batches | lr 0.00 | loss  0.46 | ppl     1.58\n",
      "| epoch  15 |  1100/ 1357 batches | lr 0.00 | loss  0.30 | ppl     1.35\n",
      "| epoch  15 |  1200/ 1357 batches | lr 0.00 | loss  0.50 | ppl     1.64\n",
      "| epoch  15 |  1300/ 1357 batches | lr 0.00 | loss  0.36 | ppl     1.43\n",
      "| epoch  16 |     0/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.26\n",
      "| epoch  16 |   100/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  16 |   200/ 1357 batches | lr 0.00 | loss  0.27 | ppl     1.31\n",
      "| epoch  16 |   300/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.29\n",
      "| epoch  16 |   400/ 1357 batches | lr 0.00 | loss  0.26 | ppl     1.30\n",
      "| epoch  16 |   500/ 1357 batches | lr 0.00 | loss  0.37 | ppl     1.45\n",
      "| epoch  16 |   600/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.18\n",
      "| epoch  16 |   700/ 1357 batches | lr 0.00 | loss  0.40 | ppl     1.49\n",
      "| epoch  16 |   800/ 1357 batches | lr 0.00 | loss  0.31 | ppl     1.36\n",
      "| epoch  16 |   900/ 1357 batches | lr 0.00 | loss  0.37 | ppl     1.45\n",
      "| epoch  16 |  1000/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.29\n",
      "| epoch  16 |  1100/ 1357 batches | lr 0.00 | loss  0.26 | ppl     1.29\n",
      "| epoch  16 |  1200/ 1357 batches | lr 0.00 | loss  0.67 | ppl     1.96\n",
      "| epoch  16 |  1300/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.19\n",
      "| epoch  17 |     0/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.16\n",
      "| epoch  17 |   100/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  17 |   200/ 1357 batches | lr 0.00 | loss  0.30 | ppl     1.35\n",
      "| epoch  17 |   300/ 1357 batches | lr 0.00 | loss  0.21 | ppl     1.24\n",
      "| epoch  17 |   400/ 1357 batches | lr 0.00 | loss  0.38 | ppl     1.47\n",
      "| epoch  17 |   500/ 1357 batches | lr 0.00 | loss  0.32 | ppl     1.37\n",
      "| epoch  17 |   600/ 1357 batches | lr 0.00 | loss  0.33 | ppl     1.39\n",
      "| epoch  17 |   700/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.26\n",
      "| epoch  17 |   800/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  17 |   900/ 1357 batches | lr 0.00 | loss  0.26 | ppl     1.29\n",
      "| epoch  17 |  1000/ 1357 batches | lr 0.00 | loss  0.36 | ppl     1.44\n",
      "| epoch  17 |  1100/ 1357 batches | lr 0.00 | loss  0.26 | ppl     1.30\n",
      "| epoch  17 |  1200/ 1357 batches | lr 0.00 | loss  0.33 | ppl     1.39\n",
      "| epoch  17 |  1300/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.26\n",
      "| epoch  18 |     0/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  18 |   100/ 1357 batches | lr 0.00 | loss  0.35 | ppl     1.41\n",
      "| epoch  18 |   200/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.19\n",
      "| epoch  18 |   300/ 1357 batches | lr 0.00 | loss  0.24 | ppl     1.27\n",
      "| epoch  18 |   400/ 1357 batches | lr 0.00 | loss  0.41 | ppl     1.51\n",
      "| epoch  18 |   500/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.25\n",
      "| epoch  18 |   600/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  18 |   700/ 1357 batches | lr 0.00 | loss  0.26 | ppl     1.29\n",
      "| epoch  18 |   800/ 1357 batches | lr 0.00 | loss  0.30 | ppl     1.35\n",
      "| epoch  18 |   900/ 1357 batches | lr 0.00 | loss  0.30 | ppl     1.35\n",
      "| epoch  18 |  1000/ 1357 batches | lr 0.00 | loss  0.56 | ppl     1.76\n",
      "| epoch  18 |  1100/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  18 |  1200/ 1357 batches | lr 0.00 | loss  0.48 | ppl     1.61\n",
      "| epoch  18 |  1300/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  19 |     0/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  19 |   100/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  19 |   200/ 1357 batches | lr 0.00 | loss  0.45 | ppl     1.57\n",
      "| epoch  19 |   300/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  19 |   400/ 1357 batches | lr 0.00 | loss  0.33 | ppl     1.38\n",
      "| epoch  19 |   500/ 1357 batches | lr 0.00 | loss  0.47 | ppl     1.60\n",
      "| epoch  19 |   600/ 1357 batches | lr 0.00 | loss  0.37 | ppl     1.45\n",
      "| epoch  19 |   700/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  19 |   800/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.16\n",
      "| epoch  19 |   900/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.28\n",
      "| epoch  19 |  1000/ 1357 batches | lr 0.00 | loss  0.48 | ppl     1.61\n",
      "| epoch  19 |  1100/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.19\n",
      "| epoch  19 |  1200/ 1357 batches | lr 0.00 | loss  0.44 | ppl     1.55\n",
      "| epoch  19 |  1300/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  20 |     0/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  20 |   100/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  20 |   200/ 1357 batches | lr 0.00 | loss  0.27 | ppl     1.31\n",
      "| epoch  20 |   300/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  20 |   400/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.06\n",
      "| epoch  20 |   500/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  20 |   600/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.19\n",
      "| epoch  20 |   700/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.16\n",
      "| epoch  20 |   800/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  20 |   900/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.26\n",
      "| epoch  20 |  1000/ 1357 batches | lr 0.00 | loss  0.51 | ppl     1.66\n",
      "| epoch  20 |  1100/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  20 |  1200/ 1357 batches | lr 0.00 | loss  0.31 | ppl     1.36\n",
      "| epoch  20 |  1300/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  21 |     0/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  21 |   100/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  21 |   200/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  21 |   300/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.25\n",
      "| epoch  21 |   400/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  21 |   500/ 1357 batches | lr 0.00 | loss  0.29 | ppl     1.33\n",
      "| epoch  21 |   600/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  21 |   700/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.28\n",
      "| epoch  21 |   800/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  21 |   900/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.25\n",
      "| epoch  21 |  1000/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.25\n",
      "| epoch  21 |  1100/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  21 |  1200/ 1357 batches | lr 0.00 | loss  0.35 | ppl     1.42\n",
      "| epoch  21 |  1300/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  22 |     0/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  22 |   100/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  22 |   200/ 1357 batches | lr 0.00 | loss  0.27 | ppl     1.31\n",
      "| epoch  22 |   300/ 1357 batches | lr 0.00 | loss  0.36 | ppl     1.43\n",
      "| epoch  22 |   400/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.21\n",
      "| epoch  22 |   500/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.21\n",
      "| epoch  22 |   600/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  22 |   700/ 1357 batches | lr 0.00 | loss  0.27 | ppl     1.31\n",
      "| epoch  22 |   800/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  22 |   900/ 1357 batches | lr 0.00 | loss  0.47 | ppl     1.60\n",
      "| epoch  22 |  1000/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  22 |  1100/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  22 |  1200/ 1357 batches | lr 0.00 | loss  0.28 | ppl     1.33\n",
      "| epoch  22 |  1300/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.25\n",
      "| epoch  23 |     0/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  23 |   100/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  23 |   200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  23 |   300/ 1357 batches | lr 0.00 | loss  0.29 | ppl     1.34\n",
      "| epoch  23 |   400/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  23 |   500/ 1357 batches | lr 0.00 | loss  0.24 | ppl     1.27\n",
      "| epoch  23 |   600/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  23 |   700/ 1357 batches | lr 0.00 | loss  0.26 | ppl     1.29\n",
      "| epoch  23 |   800/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  23 |   900/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.18\n",
      "| epoch  23 |  1000/ 1357 batches | lr 0.00 | loss  0.32 | ppl     1.38\n",
      "| epoch  23 |  1100/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  23 |  1200/ 1357 batches | lr 0.00 | loss  0.54 | ppl     1.72\n",
      "| epoch  23 |  1300/ 1357 batches | lr 0.00 | loss  0.21 | ppl     1.23\n",
      "| epoch  24 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  24 |   100/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  24 |   200/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.19\n",
      "| epoch  24 |   300/ 1357 batches | lr 0.00 | loss  0.30 | ppl     1.35\n",
      "| epoch  24 |   400/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  24 |   500/ 1357 batches | lr 0.00 | loss  0.44 | ppl     1.55\n",
      "| epoch  24 |   600/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  24 |   700/ 1357 batches | lr 0.00 | loss  0.21 | ppl     1.24\n",
      "| epoch  24 |   800/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  24 |   900/ 1357 batches | lr 0.00 | loss  0.32 | ppl     1.38\n",
      "| epoch  24 |  1000/ 1357 batches | lr 0.00 | loss  0.21 | ppl     1.23\n",
      "| epoch  24 |  1100/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  24 |  1200/ 1357 batches | lr 0.00 | loss  0.26 | ppl     1.29\n",
      "| epoch  24 |  1300/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.04\n",
      "| epoch  25 |     0/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  25 |   100/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.08\n",
      "| epoch  25 |   200/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.26\n",
      "| epoch  25 |   300/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.20\n",
      "| epoch  25 |   400/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  25 |   500/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.20\n",
      "| epoch  25 |   600/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  25 |   700/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.19\n",
      "| epoch  25 |   800/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  25 |   900/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  25 |  1000/ 1357 batches | lr 0.00 | loss  0.21 | ppl     1.24\n",
      "| epoch  25 |  1100/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  25 |  1200/ 1357 batches | lr 0.00 | loss  0.29 | ppl     1.34\n",
      "| epoch  25 |  1300/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.06\n",
      "| epoch  26 |     0/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  26 |   100/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  26 |   200/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.26\n",
      "| epoch  26 |   300/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  26 |   400/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  26 |   500/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  26 |   600/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.26\n",
      "| epoch  26 |   700/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  26 |   800/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  26 |   900/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  26 |  1000/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.28\n",
      "| epoch  26 |  1100/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  26 |  1200/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  26 |  1300/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.09\n",
      "| epoch  27 |     0/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  27 |   100/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  27 |   200/ 1357 batches | lr 0.00 | loss  0.24 | ppl     1.28\n",
      "| epoch  27 |   300/ 1357 batches | lr 0.00 | loss  0.26 | ppl     1.29\n",
      "| epoch  27 |   400/ 1357 batches | lr 0.00 | loss  0.24 | ppl     1.27\n",
      "| epoch  27 |   500/ 1357 batches | lr 0.00 | loss  0.49 | ppl     1.63\n",
      "| epoch  27 |   600/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  27 |   700/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.19\n",
      "| epoch  27 |   800/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.18\n",
      "| epoch  27 |   900/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  27 |  1000/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  27 |  1100/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  27 |  1200/ 1357 batches | lr 0.00 | loss  0.24 | ppl     1.27\n",
      "| epoch  27 |  1300/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  28 |     0/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  28 |   100/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.07\n",
      "| epoch  28 |   200/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  28 |   300/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  28 |   400/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.24\n",
      "| epoch  28 |   500/ 1357 batches | lr 0.00 | loss  0.34 | ppl     1.40\n",
      "| epoch  28 |   600/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.04\n",
      "| epoch  28 |   700/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  28 |   800/ 1357 batches | lr 0.00 | loss  0.32 | ppl     1.38\n",
      "| epoch  28 |   900/ 1357 batches | lr 0.00 | loss  0.38 | ppl     1.46\n",
      "| epoch  28 |  1000/ 1357 batches | lr 0.00 | loss  0.29 | ppl     1.33\n",
      "| epoch  28 |  1100/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.07\n",
      "| epoch  28 |  1200/ 1357 batches | lr 0.00 | loss  0.27 | ppl     1.31\n",
      "| epoch  28 |  1300/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.29\n",
      "| epoch  29 |     0/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  29 |   100/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  29 |   200/ 1357 batches | lr 0.00 | loss  0.21 | ppl     1.23\n",
      "| epoch  29 |   300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  29 |   400/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  29 |   500/ 1357 batches | lr 0.00 | loss  0.28 | ppl     1.32\n",
      "| epoch  29 |   600/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  29 |   700/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  29 |   800/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  29 |   900/ 1357 batches | lr 0.00 | loss  0.21 | ppl     1.24\n",
      "| epoch  29 |  1000/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.25\n",
      "| epoch  29 |  1100/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  29 |  1200/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.18\n",
      "| epoch  29 |  1300/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  30 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  30 |   100/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  30 |   200/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  30 |   300/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  30 |   400/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  30 |   500/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  30 |   600/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.28\n",
      "| epoch  30 |   700/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.16\n",
      "| epoch  30 |   800/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  30 |   900/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  30 |  1000/ 1357 batches | lr 0.00 | loss  0.44 | ppl     1.55\n",
      "| epoch  30 |  1100/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  30 |  1200/ 1357 batches | lr 0.00 | loss  0.26 | ppl     1.30\n",
      "| epoch  30 |  1300/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.08\n",
      "| epoch  31 |     0/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.25\n",
      "| epoch  31 |   100/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  31 |   200/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  31 |   300/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  31 |   400/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  31 |   500/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.18\n",
      "| epoch  31 |   600/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  31 |   700/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.18\n",
      "| epoch  31 |   800/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  31 |   900/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  31 |  1000/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  31 |  1100/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  31 |  1200/ 1357 batches | lr 0.00 | loss  0.31 | ppl     1.37\n",
      "| epoch  31 |  1300/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  32 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  32 |   100/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.07\n",
      "| epoch  32 |   200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  32 |   300/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  32 |   400/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.28\n",
      "| epoch  32 |   500/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.28\n",
      "| epoch  32 |   600/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  32 |   700/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  32 |   800/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.06\n",
      "| epoch  32 |   900/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  32 |  1000/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  32 |  1100/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  32 |  1200/ 1357 batches | lr 0.00 | loss  0.36 | ppl     1.43\n",
      "| epoch  32 |  1300/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  33 |     0/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  33 |   100/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.07\n",
      "| epoch  33 |   200/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  33 |   300/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.08\n",
      "| epoch  33 |   400/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  33 |   500/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  33 |   600/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  33 |   700/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  33 |   800/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.08\n",
      "| epoch  33 |   900/ 1357 batches | lr 0.00 | loss  0.44 | ppl     1.55\n",
      "| epoch  33 |  1000/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.24\n",
      "| epoch  33 |  1100/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.18\n",
      "| epoch  33 |  1200/ 1357 batches | lr 0.00 | loss  0.21 | ppl     1.24\n",
      "| epoch  33 |  1300/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.19\n",
      "| epoch  34 |     0/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.07\n",
      "| epoch  34 |   100/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.28\n",
      "| epoch  34 |   200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  34 |   300/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  34 |   400/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  34 |   500/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  34 |   600/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  34 |   700/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  34 |   800/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  34 |   900/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  34 |  1000/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  34 |  1100/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  34 |  1200/ 1357 batches | lr 0.00 | loss  0.29 | ppl     1.34\n",
      "| epoch  34 |  1300/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  35 |     0/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  35 |   100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  35 |   200/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  35 |   300/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  35 |   400/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  35 |   500/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.25\n",
      "| epoch  35 |   600/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  35 |   700/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  35 |   800/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  35 |   900/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  35 |  1000/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  35 |  1100/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  35 |  1200/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  35 |  1300/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  36 |     0/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  36 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  36 |   200/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.16\n",
      "| epoch  36 |   300/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  36 |   400/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.17\n",
      "| epoch  36 |   500/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  36 |   600/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  36 |   700/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  36 |   800/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  36 |   900/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  36 |  1000/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.12\n",
      "| epoch  36 |  1100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  36 |  1200/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.29\n",
      "| epoch  36 |  1300/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  37 |     0/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  37 |   100/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  37 |   200/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  37 |   300/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  37 |   400/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  37 |   500/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  37 |   600/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  37 |   700/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  37 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  37 |   900/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.25\n",
      "| epoch  37 |  1000/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  37 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  37 |  1200/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.28\n",
      "| epoch  37 |  1300/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  38 |     0/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  38 |   100/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  38 |   200/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.18\n",
      "| epoch  38 |   300/ 1357 batches | lr 0.00 | loss  0.35 | ppl     1.43\n",
      "| epoch  38 |   400/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.26\n",
      "| epoch  38 |   500/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.16\n",
      "| epoch  38 |   600/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  38 |   700/ 1357 batches | lr 0.00 | loss  0.42 | ppl     1.53\n",
      "| epoch  38 |   800/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.09\n",
      "| epoch  38 |   900/ 1357 batches | lr 0.00 | loss  0.21 | ppl     1.23\n",
      "| epoch  38 |  1000/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  38 |  1100/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  38 |  1200/ 1357 batches | lr 0.00 | loss  0.26 | ppl     1.30\n",
      "| epoch  38 |  1300/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  39 |     0/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.04\n",
      "| epoch  39 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  39 |   200/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  39 |   300/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  39 |   400/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  39 |   500/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.16\n",
      "| epoch  39 |   600/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  39 |   700/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.09\n",
      "| epoch  39 |   800/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  39 |   900/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  39 |  1000/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  39 |  1100/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  39 |  1200/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.26\n",
      "| epoch  39 |  1300/ 1357 batches | lr 0.00 | loss  0.34 | ppl     1.41\n",
      "| epoch  40 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  40 |   100/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  40 |   200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  40 |   300/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  40 |   400/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  40 |   500/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.21\n",
      "| epoch  40 |   600/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  40 |   700/ 1357 batches | lr 0.00 | loss  0.27 | ppl     1.31\n",
      "| epoch  40 |   800/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  40 |   900/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  40 |  1000/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  40 |  1100/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  40 |  1200/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  40 |  1300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  41 |     0/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  41 |   100/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  41 |   200/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  41 |   300/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  41 |   400/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.06\n",
      "| epoch  41 |   500/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.12\n",
      "| epoch  41 |   600/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  41 |   700/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  41 |   800/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  41 |   900/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  41 |  1000/ 1357 batches | lr 0.00 | loss  0.37 | ppl     1.45\n",
      "| epoch  41 |  1100/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.18\n",
      "| epoch  41 |  1200/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.19\n",
      "| epoch  41 |  1300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  42 |     0/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  42 |   100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  42 |   200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  42 |   300/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  42 |   400/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.09\n",
      "| epoch  42 |   500/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.16\n",
      "| epoch  42 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  42 |   700/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  42 |   800/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  42 |   900/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.06\n",
      "| epoch  42 |  1000/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.20\n",
      "| epoch  42 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  42 |  1200/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.18\n",
      "| epoch  42 |  1300/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  43 |     0/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  43 |   100/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  43 |   200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.12\n",
      "| epoch  43 |   300/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  43 |   400/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  43 |   500/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  43 |   600/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  43 |   700/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.17\n",
      "| epoch  43 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  43 |   900/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  43 |  1000/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.12\n",
      "| epoch  43 |  1100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  43 |  1200/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  43 |  1300/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  44 |     0/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  44 |   100/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  44 |   200/ 1357 batches | lr 0.00 | loss  0.28 | ppl     1.32\n",
      "| epoch  44 |   300/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.07\n",
      "| epoch  44 |   400/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  44 |   500/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  44 |   600/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  44 |   700/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  44 |   800/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  44 |   900/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  44 |  1000/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  44 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  44 |  1200/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.24\n",
      "| epoch  44 |  1300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  45 |     0/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  45 |   100/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  45 |   200/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  45 |   300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  45 |   400/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  45 |   500/ 1357 batches | lr 0.00 | loss  0.28 | ppl     1.32\n",
      "| epoch  45 |   600/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  45 |   700/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  45 |   800/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  45 |   900/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  45 |  1000/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  45 |  1100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  45 |  1200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.12\n",
      "| epoch  45 |  1300/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  46 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  46 |   100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  46 |   200/ 1357 batches | lr 0.00 | loss  0.33 | ppl     1.39\n",
      "| epoch  46 |   300/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  46 |   400/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  46 |   500/ 1357 batches | lr 0.00 | loss  0.27 | ppl     1.30\n",
      "| epoch  46 |   600/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  46 |   700/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  46 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  46 |   900/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  46 |  1000/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  46 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  46 |  1200/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.16\n",
      "| epoch  46 |  1300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  47 |     0/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  47 |   100/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  47 |   200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  47 |   300/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  47 |   400/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.07\n",
      "| epoch  47 |   500/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.29\n",
      "| epoch  47 |   600/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  47 |   700/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  47 |   800/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  47 |   900/ 1357 batches | lr 0.00 | loss  0.24 | ppl     1.27\n",
      "| epoch  47 |  1000/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  47 |  1100/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  47 |  1200/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.25\n",
      "| epoch  47 |  1300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  48 |     0/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  48 |   100/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  48 |   200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  48 |   300/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  48 |   400/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  48 |   500/ 1357 batches | lr 0.00 | loss  0.27 | ppl     1.32\n",
      "| epoch  48 |   600/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.08\n",
      "| epoch  48 |   700/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  48 |   800/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  48 |   900/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  48 |  1000/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.18\n",
      "| epoch  48 |  1100/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  48 |  1200/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.28\n",
      "| epoch  48 |  1300/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  49 |     0/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  49 |   100/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  49 |   200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  49 |   300/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  49 |   400/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  49 |   500/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  49 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  49 |   700/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  49 |   800/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  49 |   900/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  49 |  1000/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  49 |  1100/ 1357 batches | lr 0.00 | loss  0.27 | ppl     1.31\n",
      "| epoch  49 |  1200/ 1357 batches | lr 0.00 | loss  0.34 | ppl     1.41\n",
      "| epoch  49 |  1300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  50 |     0/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  50 |   100/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  50 |   200/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.14\n",
      "| epoch  50 |   300/ 1357 batches | lr 0.00 | loss  0.21 | ppl     1.24\n",
      "| epoch  50 |   400/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  50 |   500/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.24\n",
      "| epoch  50 |   600/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.24\n",
      "| epoch  50 |   700/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  50 |   800/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  50 |   900/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  50 |  1000/ 1357 batches | lr 0.00 | loss  0.21 | ppl     1.23\n",
      "| epoch  50 |  1100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  50 |  1200/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.25\n",
      "| epoch  50 |  1300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  51 |     0/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  51 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  51 |   200/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.17\n",
      "| epoch  51 |   300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  51 |   400/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.20\n",
      "| epoch  51 |   500/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  51 |   600/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  51 |   700/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.16\n",
      "| epoch  51 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  51 |   900/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  51 |  1000/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  51 |  1100/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.09\n",
      "| epoch  51 |  1200/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.17\n",
      "| epoch  51 |  1300/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.09\n",
      "| epoch  52 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  52 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  52 |   200/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.21\n",
      "| epoch  52 |   300/ 1357 batches | lr 0.00 | loss  0.21 | ppl     1.23\n",
      "| epoch  52 |   400/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  52 |   500/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  52 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  52 |   700/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  52 |   800/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.04\n",
      "| epoch  52 |   900/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  52 |  1000/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.12\n",
      "| epoch  52 |  1100/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  52 |  1200/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  52 |  1300/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  53 |     0/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  53 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  53 |   200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.12\n",
      "| epoch  53 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  53 |   400/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.28\n",
      "| epoch  53 |   500/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.19\n",
      "| epoch  53 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  53 |   700/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  53 |   800/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  53 |   900/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.08\n",
      "| epoch  53 |  1000/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.24\n",
      "| epoch  53 |  1100/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  53 |  1200/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  53 |  1300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  54 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  54 |   100/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.04\n",
      "| epoch  54 |   200/ 1357 batches | lr 0.00 | loss  0.28 | ppl     1.32\n",
      "| epoch  54 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  54 |   400/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  54 |   500/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  54 |   600/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  54 |   700/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  54 |   800/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  54 |   900/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  54 |  1000/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  54 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  54 |  1200/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  54 |  1300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  55 |     0/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  55 |   100/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.07\n",
      "| epoch  55 |   200/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  55 |   300/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  55 |   400/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  55 |   500/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  55 |   600/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  55 |   700/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  55 |   800/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  55 |   900/ 1357 batches | lr 0.00 | loss  0.24 | ppl     1.27\n",
      "| epoch  55 |  1000/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  55 |  1100/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  55 |  1200/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.24\n",
      "| epoch  55 |  1300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  56 |     0/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  56 |   100/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  56 |   200/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.16\n",
      "| epoch  56 |   300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  56 |   400/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  56 |   500/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.20\n",
      "| epoch  56 |   600/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.20\n",
      "| epoch  56 |   700/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  56 |   800/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  56 |   900/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.18\n",
      "| epoch  56 |  1000/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  56 |  1100/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  56 |  1200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  56 |  1300/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  57 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  57 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  57 |   200/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.13\n",
      "| epoch  57 |   300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  57 |   400/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.21\n",
      "| epoch  57 |   500/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  57 |   600/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  57 |   700/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.21\n",
      "| epoch  57 |   800/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  57 |   900/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.08\n",
      "| epoch  57 |  1000/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.20\n",
      "| epoch  57 |  1100/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  57 |  1200/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  57 |  1300/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  58 |     0/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  58 |   100/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  58 |   200/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.18\n",
      "| epoch  58 |   300/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  58 |   400/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  58 |   500/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.20\n",
      "| epoch  58 |   600/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  58 |   700/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  58 |   800/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  58 |   900/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  58 |  1000/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  58 |  1100/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  58 |  1200/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  58 |  1300/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  59 |     0/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  59 |   100/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.26\n",
      "| epoch  59 |   200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.12\n",
      "| epoch  59 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  59 |   400/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  59 |   500/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.29\n",
      "| epoch  59 |   600/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  59 |   700/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.08\n",
      "| epoch  59 |   800/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  59 |   900/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.03\n",
      "| epoch  59 |  1000/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  59 |  1100/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  59 |  1200/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.20\n",
      "| epoch  59 |  1300/ 1357 batches | lr 0.00 | loss  0.24 | ppl     1.27\n",
      "| epoch  60 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  60 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  60 |   200/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  60 |   300/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.23\n",
      "| epoch  60 |   400/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.21\n",
      "| epoch  60 |   500/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  60 |   600/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.21\n",
      "| epoch  60 |   700/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.25\n",
      "| epoch  60 |   800/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  60 |   900/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  60 |  1000/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  60 |  1100/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  60 |  1200/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  60 |  1300/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  61 |     0/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  61 |   100/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.09\n",
      "| epoch  61 |   200/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.16\n",
      "| epoch  61 |   300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  61 |   400/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.26\n",
      "| epoch  61 |   500/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.20\n",
      "| epoch  61 |   600/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  61 |   700/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  61 |   800/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  61 |   900/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.19\n",
      "| epoch  61 |  1000/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.21\n",
      "| epoch  61 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  61 |  1200/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  61 |  1300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  62 |     0/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  62 |   100/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.21\n",
      "| epoch  62 |   200/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  62 |   300/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  62 |   400/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  62 |   500/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.12\n",
      "| epoch  62 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  62 |   700/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.20\n",
      "| epoch  62 |   800/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.23\n",
      "| epoch  62 |   900/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.29\n",
      "| epoch  62 |  1000/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.16\n",
      "| epoch  62 |  1100/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  62 |  1200/ 1357 batches | lr 0.00 | loss  0.44 | ppl     1.56\n",
      "| epoch  62 |  1300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  63 |     0/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  63 |   100/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  63 |   200/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  63 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  63 |   400/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  63 |   500/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.16\n",
      "| epoch  63 |   600/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  63 |   700/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  63 |   800/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  63 |   900/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  63 |  1000/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  63 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  63 |  1200/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  63 |  1300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  64 |     0/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  64 |   100/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  64 |   200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  64 |   300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  64 |   400/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.08\n",
      "| epoch  64 |   500/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.21\n",
      "| epoch  64 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  64 |   700/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  64 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  64 |   900/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  64 |  1000/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  64 |  1100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  64 |  1200/ 1357 batches | lr 0.00 | loss  0.30 | ppl     1.35\n",
      "| epoch  64 |  1300/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  65 |     0/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  65 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  65 |   200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  65 |   300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  65 |   400/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  65 |   500/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  65 |   600/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  65 |   700/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  65 |   800/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  65 |   900/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  65 |  1000/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  65 |  1100/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.09\n",
      "| epoch  65 |  1200/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  65 |  1300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  66 |     0/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  66 |   100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  66 |   200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  66 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  66 |   400/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  66 |   500/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.13\n",
      "| epoch  66 |   600/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  66 |   700/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  66 |   800/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  66 |   900/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  66 |  1000/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  66 |  1100/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  66 |  1200/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  66 |  1300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  67 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  67 |   100/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  67 |   200/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.28\n",
      "| epoch  67 |   300/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  67 |   400/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  67 |   500/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.18\n",
      "| epoch  67 |   600/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  67 |   700/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  67 |   800/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  67 |   900/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.26\n",
      "| epoch  67 |  1000/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  67 |  1100/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  67 |  1200/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.09\n",
      "| epoch  67 |  1300/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  68 |     0/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  68 |   100/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.20\n",
      "| epoch  68 |   200/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.19\n",
      "| epoch  68 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  68 |   400/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  68 |   500/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  68 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  68 |   700/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.08\n",
      "| epoch  68 |   800/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  68 |   900/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  68 |  1000/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.13\n",
      "| epoch  68 |  1100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  68 |  1200/ 1357 batches | lr 0.00 | loss  0.36 | ppl     1.43\n",
      "| epoch  68 |  1300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  69 |     0/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  69 |   100/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  69 |   200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  69 |   300/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  69 |   400/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  69 |   500/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  69 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  69 |   700/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  69 |   800/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  69 |   900/ 1357 batches | lr 0.00 | loss  0.35 | ppl     1.42\n",
      "| epoch  69 |  1000/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  69 |  1100/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  69 |  1200/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  69 |  1300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  70 |     0/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  70 |   100/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  70 |   200/ 1357 batches | lr 0.00 | loss  0.33 | ppl     1.39\n",
      "| epoch  70 |   300/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  70 |   400/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  70 |   500/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  70 |   600/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  70 |   700/ 1357 batches | lr 0.00 | loss  0.27 | ppl     1.31\n",
      "| epoch  70 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  70 |   900/ 1357 batches | lr 0.00 | loss  0.28 | ppl     1.32\n",
      "| epoch  70 |  1000/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.17\n",
      "| epoch  70 |  1100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  70 |  1200/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  70 |  1300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  71 |     0/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  71 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  71 |   200/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  71 |   300/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  71 |   400/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  71 |   500/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.21\n",
      "| epoch  71 |   600/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  71 |   700/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  71 |   800/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  71 |   900/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  71 |  1000/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  71 |  1100/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  71 |  1200/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  71 |  1300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  72 |     0/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  72 |   100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  72 |   200/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  72 |   300/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.06\n",
      "| epoch  72 |   400/ 1357 batches | lr 0.00 | loss  0.23 | ppl     1.26\n",
      "| epoch  72 |   500/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  72 |   600/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  72 |   700/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.06\n",
      "| epoch  72 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  72 |   900/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  72 |  1000/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.20\n",
      "| epoch  72 |  1100/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.06\n",
      "| epoch  72 |  1200/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  72 |  1300/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.08\n",
      "| epoch  73 |     0/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  73 |   100/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  73 |   200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  73 |   300/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  73 |   400/ 1357 batches | lr 0.00 | loss  0.29 | ppl     1.33\n",
      "| epoch  73 |   500/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  73 |   600/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  73 |   700/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  73 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  73 |   900/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  73 |  1000/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  73 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  73 |  1200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  73 |  1300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  74 |     0/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  74 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  74 |   200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  74 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  74 |   400/ 1357 batches | lr 0.00 | loss  0.28 | ppl     1.32\n",
      "| epoch  74 |   500/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  74 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  74 |   700/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  74 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  74 |   900/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  74 |  1000/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  74 |  1100/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  74 |  1200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  74 |  1300/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  75 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  75 |   100/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  75 |   200/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  75 |   300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  75 |   400/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  75 |   500/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  75 |   600/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.05\n",
      "| epoch  75 |   700/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  75 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  75 |   900/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  75 |  1000/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  75 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  75 |  1200/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  75 |  1300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  76 |     0/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  76 |   100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  76 |   200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  76 |   300/ 1357 batches | lr 0.00 | loss  0.30 | ppl     1.34\n",
      "| epoch  76 |   400/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  76 |   500/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  76 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  76 |   700/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  76 |   800/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  76 |   900/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  76 |  1000/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.13\n",
      "| epoch  76 |  1100/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  76 |  1200/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  76 |  1300/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  77 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  77 |   100/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.06\n",
      "| epoch  77 |   200/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.18\n",
      "| epoch  77 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  77 |   400/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  77 |   500/ 1357 batches | lr 0.00 | loss  0.36 | ppl     1.44\n",
      "| epoch  77 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  77 |   700/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  77 |   800/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  77 |   900/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  77 |  1000/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.19\n",
      "| epoch  77 |  1100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  77 |  1200/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.13\n",
      "| epoch  77 |  1300/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  78 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  78 |   100/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  78 |   200/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  78 |   300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  78 |   400/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.07\n",
      "| epoch  78 |   500/ 1357 batches | lr 0.00 | loss  0.24 | ppl     1.27\n",
      "| epoch  78 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  78 |   700/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  78 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  78 |   900/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  78 |  1000/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  78 |  1100/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  78 |  1200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  78 |  1300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  79 |     0/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  79 |   100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  79 |   200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.12\n",
      "| epoch  79 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  79 |   400/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  79 |   500/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  79 |   600/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  79 |   700/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.12\n",
      "| epoch  79 |   800/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  79 |   900/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  79 |  1000/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.24\n",
      "| epoch  79 |  1100/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  79 |  1200/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  79 |  1300/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  80 |     0/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  80 |   100/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  80 |   200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  80 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  80 |   400/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  80 |   500/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  80 |   600/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  80 |   700/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  80 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  80 |   900/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  80 |  1000/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  80 |  1100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  80 |  1200/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  80 |  1300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  81 |     0/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  81 |   100/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  81 |   200/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  81 |   300/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  81 |   400/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  81 |   500/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  81 |   600/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  81 |   700/ 1357 batches | lr 0.00 | loss  0.19 | ppl     1.20\n",
      "| epoch  81 |   800/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  81 |   900/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  81 |  1000/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  81 |  1100/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  81 |  1200/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  81 |  1300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  82 |     0/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  82 |   100/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  82 |   200/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  82 |   300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  82 |   400/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  82 |   500/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  82 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  82 |   700/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  82 |   800/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  82 |   900/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  82 |  1000/ 1357 batches | lr 0.00 | loss  0.29 | ppl     1.34\n",
      "| epoch  82 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  82 |  1200/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.20\n",
      "| epoch  82 |  1300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  83 |     0/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  83 |   100/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  83 |   200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  83 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  83 |   400/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  83 |   500/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.19\n",
      "| epoch  83 |   600/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  83 |   700/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  83 |   800/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  83 |   900/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  83 |  1000/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.13\n",
      "| epoch  83 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  83 |  1200/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  83 |  1300/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  84 |     0/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  84 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  84 |   200/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  84 |   300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  84 |   400/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  84 |   500/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  84 |   600/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  84 |   700/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  84 |   800/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  84 |   900/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  84 |  1000/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  84 |  1100/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  84 |  1200/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  84 |  1300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  85 |     0/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  85 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  85 |   200/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.17\n",
      "| epoch  85 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  85 |   400/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  85 |   500/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  85 |   600/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  85 |   700/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  85 |   800/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  85 |   900/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  85 |  1000/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  85 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  85 |  1200/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  85 |  1300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  86 |     0/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  86 |   100/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  86 |   200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.12\n",
      "| epoch  86 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  86 |   400/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  86 |   500/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  86 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  86 |   700/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  86 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  86 |   900/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  86 |  1000/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  86 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  86 |  1200/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.24\n",
      "| epoch  86 |  1300/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  87 |     0/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  87 |   100/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  87 |   200/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  87 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  87 |   400/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  87 |   500/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  87 |   600/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  87 |   700/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  87 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  87 |   900/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  87 |  1000/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  87 |  1100/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  87 |  1200/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  87 |  1300/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  88 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  88 |   100/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  88 |   200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  88 |   300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  88 |   400/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.07\n",
      "| epoch  88 |   500/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  88 |   600/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  88 |   700/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  88 |   800/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  88 |   900/ 1357 batches | lr 0.00 | loss  0.38 | ppl     1.47\n",
      "| epoch  88 |  1000/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  88 |  1100/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  88 |  1200/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  88 |  1300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  89 |     0/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  89 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  89 |   200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  89 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  89 |   400/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  89 |   500/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  89 |   600/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  89 |   700/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.07\n",
      "| epoch  89 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  89 |   900/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  89 |  1000/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  89 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  89 |  1200/ 1357 batches | lr 0.00 | loss  0.26 | ppl     1.30\n",
      "| epoch  89 |  1300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  90 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  90 |   100/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  90 |   200/ 1357 batches | lr 0.00 | loss  0.20 | ppl     1.22\n",
      "| epoch  90 |   300/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  90 |   400/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  90 |   500/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  90 |   600/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  90 |   700/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  90 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  90 |   900/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  90 |  1000/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.12\n",
      "| epoch  90 |  1100/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  90 |  1200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  90 |  1300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  91 |     0/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  91 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  91 |   200/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.20\n",
      "| epoch  91 |   300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  91 |   400/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  91 |   500/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  91 |   600/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  91 |   700/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  91 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  91 |   900/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  91 |  1000/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  91 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  91 |  1200/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.19\n",
      "| epoch  91 |  1300/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  92 |     0/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  92 |   100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  92 |   200/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  92 |   300/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  92 |   400/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  92 |   500/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  92 |   600/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  92 |   700/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  92 |   800/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  92 |   900/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  92 |  1000/ 1357 batches | lr 0.00 | loss  0.17 | ppl     1.18\n",
      "| epoch  92 |  1100/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  92 |  1200/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  92 |  1300/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  93 |     0/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  93 |   100/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  93 |   200/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  93 |   300/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  93 |   400/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  93 |   500/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  93 |   600/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  93 |   700/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  93 |   800/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  93 |   900/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  93 |  1000/ 1357 batches | lr 0.00 | loss  0.18 | ppl     1.19\n",
      "| epoch  93 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  93 |  1200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  93 |  1300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  94 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  94 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  94 |   200/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.28\n",
      "| epoch  94 |   300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  94 |   400/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.05\n",
      "| epoch  94 |   500/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  94 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  94 |   700/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  94 |   800/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  94 |   900/ 1357 batches | lr 0.00 | loss  0.22 | ppl     1.25\n",
      "| epoch  94 |  1000/ 1357 batches | lr 0.00 | loss  0.30 | ppl     1.35\n",
      "| epoch  94 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  94 |  1200/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.11\n",
      "| epoch  94 |  1300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  95 |     0/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  95 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  95 |   200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  95 |   300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  95 |   400/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  95 |   500/ 1357 batches | lr 0.00 | loss  0.28 | ppl     1.32\n",
      "| epoch  95 |   600/ 1357 batches | lr 0.00 | loss  0.05 | ppl     1.06\n",
      "| epoch  95 |   700/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  95 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  95 |   900/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  95 |  1000/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.11\n",
      "| epoch  95 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  95 |  1200/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  95 |  1300/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  96 |     0/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  96 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  96 |   200/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  96 |   300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  96 |   400/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  96 |   500/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  96 |   600/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.08\n",
      "| epoch  96 |   700/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  96 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  96 |   900/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.08\n",
      "| epoch  96 |  1000/ 1357 batches | lr 0.00 | loss  0.11 | ppl     1.12\n",
      "| epoch  96 |  1100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  96 |  1200/ 1357 batches | lr 0.00 | loss  0.15 | ppl     1.16\n",
      "| epoch  96 |  1300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  97 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  97 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  97 |   200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  97 |   300/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  97 |   400/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  97 |   500/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  97 |   600/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  97 |   700/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  97 |   800/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  97 |   900/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.09\n",
      "| epoch  97 |  1000/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  97 |  1100/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  97 |  1200/ 1357 batches | lr 0.00 | loss  0.25 | ppl     1.28\n",
      "| epoch  97 |  1300/ 1357 batches | lr 0.00 | loss  0.03 | ppl     1.03\n",
      "| epoch  98 |     0/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  98 |   100/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  98 |   200/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  98 |   300/ 1357 batches | lr 0.00 | loss  0.04 | ppl     1.04\n",
      "| epoch  98 |   400/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  98 |   500/ 1357 batches | lr 0.00 | loss  0.13 | ppl     1.14\n",
      "| epoch  98 |   600/ 1357 batches | lr 0.00 | loss  0.00 | ppl     1.00\n",
      "| epoch  98 |   700/ 1357 batches | lr 0.00 | loss  0.06 | ppl     1.06\n",
      "| epoch  98 |   800/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  98 |   900/ 1357 batches | lr 0.00 | loss  0.07 | ppl     1.07\n",
      "| epoch  98 |  1000/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n",
      "| epoch  98 |  1100/ 1357 batches | lr 0.00 | loss  0.27 | ppl     1.31\n",
      "| epoch  98 |  1200/ 1357 batches | lr 0.00 | loss  0.27 | ppl     1.31\n",
      "| epoch  98 |  1300/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  99 |     0/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  99 |   100/ 1357 batches | lr 0.00 | loss  0.14 | ppl     1.15\n",
      "| epoch  99 |   200/ 1357 batches | lr 0.00 | loss  0.16 | ppl     1.17\n",
      "| epoch  99 |   300/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  99 |   400/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  99 |   500/ 1357 batches | lr 0.00 | loss  0.09 | ppl     1.10\n",
      "| epoch  99 |   600/ 1357 batches | lr 0.00 | loss  0.01 | ppl     1.01\n",
      "| epoch  99 |   700/ 1357 batches | lr 0.00 | loss  0.08 | ppl     1.08\n",
      "| epoch  99 |   800/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  99 |   900/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  99 |  1000/ 1357 batches | lr 0.00 | loss  0.10 | ppl     1.10\n",
      "| epoch  99 |  1100/ 1357 batches | lr 0.00 | loss  0.02 | ppl     1.02\n",
      "| epoch  99 |  1200/ 1357 batches | lr 0.00 | loss  0.29 | ppl     1.34\n",
      "| epoch  99 |  1300/ 1357 batches | lr 0.00 | loss  0.12 | ppl     1.13\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSTM_Model(\n",
      "  (lstm): LSTM(300, 600, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=600, out_features=28, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_0.parameters())\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "train_metrics, val_metrics = dl_train(model=model_0, loader=train_dataloader_0, criterion=criterion, optimizer=optimizer, \n",
    "                                   epoches=NUM_EPOCH, val=False, val_loader=None,\n",
    "                                   device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_0.state_dict(), r'LSTM_Model_1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3275.898477649782\n",
      "135700\n",
      "0.2312643602490425\n"
     ]
    }
   ],
   "source": [
    "print(train_metrics)\n",
    "print(len(val_metrics))\n",
    "l = len(val_metrics)\n",
    "print(np.mean(val_metrics[l//5:l//5+20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "etrain_loader = DataLoader(dataset=dataset_0, batch_size=1)\n",
    "def self_examine(model):\n",
    "    model = model.to(device)\n",
    "    cnt1 = 0\n",
    "    cnt2 = 0\n",
    "    length = len(etrain_loader)\n",
    "    for i,(x,y) in enumerate(etrain_loader):\n",
    "        if i == 0 :\n",
    "            print(x)\n",
    "            print(y)\n",
    "        with torch.no_grad():\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pre = model(x)\n",
    "            if torch.argmax(y_pre) == y:\n",
    "                cnt1+=1\n",
    "            if i == 0:\n",
    "                print(y)\n",
    "                print(torch.topk(y_pre, 5)[1])\n",
    "            if y in torch.topk(y_pre, 5)[1]:\n",
    "                cnt2 += 1\n",
    "    acc1 = cnt1/length\n",
    "    acc2 = cnt2/length\n",
    "    print(f'top1 accuracy: {acc1:.3f}')\n",
    "    print(f'top5 accuracy: {acc2:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_0 = lstm_model_dataset(data_test)\n",
    "test_loader = DataLoader(test_dataset_0, batch_size=1)\n",
    "def test_examine(model):\n",
    "    model = model.to(device)\n",
    "    cnt1 = 0\n",
    "    cnt2 = 0\n",
    "    length = len(test_loader)\n",
    "    for i,(x,y) in enumerate(test_loader):\n",
    "        if i == 0 :\n",
    "            print(x)\n",
    "            print(y)\n",
    "        with torch.no_grad():\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pre = model(x)\n",
    "            if torch.argmax(y_pre) == y:\n",
    "                cnt1+=1\n",
    "            if i == 0:\n",
    "                print(y)\n",
    "                print(torch.topk(y_pre, 5)[1])\n",
    "            if y in torch.topk(y_pre, 5)[1]:\n",
    "                cnt2 += 1\n",
    "    acc1 = cnt1/length\n",
    "    acc2 = cnt2/length\n",
    "    print(f'top1 accuracy: {acc1:.3f}')\n",
    "    print(f'top5 accuracy: {acc2:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0962, -0.0287, -0.1084,  ...,  0.0693,  0.1045, -0.1641],\n",
      "         [-0.0019,  0.2021,  0.1514,  ..., -0.0540, -0.0693, -0.1348],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([25])\n",
      "tensor([25], device='cuda:0')\n",
      "tensor([[25, 24, 15,  5, 10]], device='cuda:0')\n",
      "top1 accuracy: 0.420\n",
      "top5 accuracy: 0.744\n"
     ]
    }
   ],
   "source": [
    "test_examine(model_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0535,  0.1309,  0.2656,  ..., -0.0042,  0.2197, -0.1777],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.1816,  0.1650, -0.1660,  ...,  0.1592,  0.1279,  0.0022],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([27])\n",
      "tensor([27], device='cuda:0')\n",
      "tensor([[27, 26, 25, 16,  9]], device='cuda:0')\n",
      "top1 accuracy: 0.978\n",
      "top5 accuracy: 0.997\n"
     ]
    }
   ],
   "source": [
    "self_examine(model_0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lstm_1:\n",
    "- start loss: 2.5274\n",
    "- 20 epoch loss: 0.2312\n",
    "- 50 epoch loss: 0.0748\n",
    "- 100 epoch loss: 0.0645\n",
    "- lr scheduler: None\n",
    "- embedding: wv(pretrain)\n",
    "- tokenize: word\n",
    "- train(top1 accuracy: 0.978\n",
    "top5 accuracy: 0.997)\n",
    "- test(top1 accuracy: 0.420\n",
    "top5 accuracy: 0.744)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\songy\\AppData\\Local\\Temp\\ipykernel_27244\\2069292270.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  batch_x = torch.tensor(batch_x).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100, loss: 3.98350\n",
      "iter: 200, loss: 3.63891\n",
      "iter: 300, loss: 4.32641\n",
      "iter: 400, loss: 3.30978\n",
      "iter: 500, loss: 3.97775\n",
      "iter: 600, loss: 3.65365\n",
      "iter: 700, loss: 3.96787\n",
      "iter: 800, loss: 3.64376\n",
      "iter: 900, loss: 3.64869\n",
      "iter: 1000, loss: 3.65364\n",
      "iter: 1100, loss: 3.98264\n",
      "iter: 1200, loss: 3.98263\n",
      "iter: 1300, loss: 3.30492\n",
      "iter: 1400, loss: 4.30180\n",
      "iter: 1500, loss: 3.64870\n",
      "iter: 1600, loss: 3.97771\n",
      "iter: 1700, loss: 3.97771\n",
      "iter: 1800, loss: 3.97771\n",
      "iter: 1900, loss: 3.65363\n",
      "iter: 2000, loss: 3.97771\n",
      "iter: 2100, loss: 3.97771\n",
      "iter: 2200, loss: 3.64870\n",
      "iter: 2300, loss: 3.65855\n",
      "iter: 2400, loss: 4.31165\n",
      "iter: 2500, loss: 3.65362\n",
      "iter: 2600, loss: 3.64377\n",
      "iter: 2700, loss: 3.31476\n",
      "iter: 2800, loss: 4.64559\n",
      "iter: 2900, loss: 3.31475\n",
      "iter: 3000, loss: 4.66037\n",
      "iter: 3100, loss: 4.65544\n",
      "iter: 3200, loss: 4.96475\n",
      "iter: 3300, loss: 3.64377\n",
      "iter: 3400, loss: 4.64559\n",
      "iter: 3500, loss: 4.29687\n",
      "iter: 3600, loss: 3.30490\n",
      "iter: 3700, loss: 3.63884\n",
      "iter: 3800, loss: 3.64377\n",
      "iter: 3900, loss: 3.30983\n",
      "iter: 4000, loss: 3.65855\n",
      "iter: 4100, loss: 3.95800\n",
      "iter: 4200, loss: 3.30490\n",
      "iter: 4300, loss: 4.65051\n",
      "iter: 4400, loss: 3.64870\n",
      "iter: 4500, loss: 3.99249\n",
      "iter: 4600, loss: 3.98264\n",
      "iter: 4700, loss: 3.96785\n",
      "iter: 4800, loss: 4.64559\n",
      "iter: 4900, loss: 4.97953\n",
      "iter: 5000, loss: 3.31968\n",
      "iter: 5100, loss: 3.64377\n",
      "iter: 5200, loss: 4.30672\n",
      "iter: 5300, loss: 4.63573\n",
      "iter: 5400, loss: 3.31968\n",
      "iter: 5500, loss: 3.98264\n",
      "iter: 5600, loss: 3.98756\n",
      "iter: 5700, loss: 3.30983\n",
      "iter: 5800, loss: 3.63884\n",
      "iter: 5900, loss: 3.98756\n",
      "iter: 6000, loss: 3.99249\n",
      "iter: 6100, loss: 3.98264\n",
      "iter: 6200, loss: 4.32150\n",
      "iter: 6300, loss: 3.30983\n",
      "iter: 6400, loss: 3.97278\n",
      "iter: 6500, loss: 3.30490\n",
      "iter: 6600, loss: 3.98264\n",
      "iter: 6700, loss: 3.98756\n",
      "iter: 6800, loss: 3.98756\n",
      "iter: 6900, loss: 3.97771\n",
      "iter: 7000, loss: 3.65362\n",
      "iter: 7100, loss: 3.32461\n",
      "iter: 7200, loss: 3.97771\n",
      "iter: 7300, loss: 3.63884\n",
      "iter: 7400, loss: 4.63573\n",
      "iter: 7500, loss: 3.63392\n",
      "iter: 7600, loss: 3.97771\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m     seq\u001b[39m.\u001b[39mappend(encodding)\n\u001b[0;32m     26\u001b[0m   batch_x\u001b[39m.\u001b[39mappend(seq)\n\u001b[1;32m---> 27\u001b[0m batch_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(batch_x)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     29\u001b[0m batch_y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(BATCH_SIZE, NUM_CLASSES)\n\u001b[0;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m idx,each \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batch_y_label):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=LR)\n",
    "iter=0\n",
    "for epochs in range(NUM_EPOCH):\n",
    "  for i,batch in enumerate (data_train_batch):\n",
    "    batch_x_text = [x['text'] for x in batch]\n",
    "    batch_y_label = [y['labels'] for y in batch]\n",
    "    batch_x = []\n",
    "    for each in batch_x_text:\n",
    "      words = each.split()\n",
    "      if len(words) > SEQ_LEN:\n",
    "        words = words[:SEQ_LEN]\n",
    "      elif len(words) < SEQ_LEN:\n",
    "        words.extend(['</s>'] * (SEQ_LEN-len(words)))\n",
    "      assert len(words) == SEQ_LEN\n",
    "      seq = []\n",
    "      for w in words:\n",
    "        if w in wv:\n",
    "          encodding = wv[w]\n",
    "        else:\n",
    "          # print(f'{w} is not in word_dict!')\n",
    "          encodding = wv['</s>']\n",
    "        seq.append(encodding)\n",
    "      batch_x.append(seq)\n",
    "    batch_x = torch.tensor(batch_x).to(device)\n",
    "\n",
    "    batch_y = torch.zeros(BATCH_SIZE, NUM_CLASSES)\n",
    "    for idx,each in enumerate(batch_y_label):\n",
    "      batch_y[idx][each] = 1\n",
    "    batch_y = batch_y.to(device)\n",
    "\n",
    "    outputs = model(batch_x)\n",
    "    outputs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    # if iter % 100 == 0:\n",
    "      # print(outputs) \n",
    "    optimizer.zero_grad()   # 将每次传播时的梯度累积清除\n",
    "    # print(outputs.shape, batch_y.shape)\n",
    "    loss = criterion(outputs,batch_y) # 计算损失\n",
    "    loss.backward() # 反向传播\n",
    "    optimizer.step()\n",
    "    iter+=1\n",
    "    if iter % 100 == 0:\n",
    "      print(\"iter: %d, loss: %1.5f\" % (iter, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.nn.LSTM(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS,bidirectional=False)\n",
    "input = torch.randn(5, 3, 10) #(seq_len, batch, input_size)\n",
    "h0 = torch.randn(300, 3, 20) #(num_layers,batch,output_size)\n",
    "c0 = torch.randn(4, 3, 20) #(num_layers,batch,output_size)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
